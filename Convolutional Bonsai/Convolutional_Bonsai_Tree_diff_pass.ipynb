{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "source": [
    "# CIFAR - 10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = './cifar10/'\n",
    "Xtrain = np.load(dirc + 'Xtrain.npy').reshape(-1,32*32*3)\n",
    "Ytrain = np.load(dirc + 'Ytrain.npy')\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = tts(Xtrain, Ytrain, stratify = Ytrain, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 37500 ,Data Dims: 3072 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 420 # index of data object....\n",
    "obj = Xtest[i].reshape(32,32,3)\n",
    "plt.imshow(np.rollaxis(obj,1,0),cmap='gray')\n",
    "plt.show()\n",
    "print(Ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, kernelsshp, strides, cDepth = 2, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.cDepth = cDepth\n",
    "        self.ciNodes = 2**self.cDepth - 1\n",
    "        self.ctNodes = 2*self.ciNodes + 1\n",
    "        \n",
    "        \n",
    "        self.kernelsT = []\n",
    "        \n",
    "        self.strides = []\n",
    "        \n",
    "        self.channels = 3\n",
    "        self.d1 = 32\n",
    "        self.d2 = 32\n",
    "        d1 = self.d1\n",
    "        d2 = self.d2\n",
    "        oD1 = d1\n",
    "        oD2 = d2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wts = []\n",
    "        self.wts1 = []\n",
    "        self.wts2 = []\n",
    "        self.bs = []\n",
    "        \n",
    "        \n",
    "        h = 0\n",
    "        h_old = 0\n",
    "        Codims1 = self.d1\n",
    "        Codims2 = self.d2\n",
    "        for i in range(self.ctNodes):\n",
    "            \n",
    "            h = int(np.floor(np.log(i+1)/np.log(2)))\n",
    "            \n",
    "            self.kernelsT.append(\n",
    "                tf.get_variable('kernelT'+str(i), kernelsshp[h], \n",
    "                             initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32),\n",
    "                             dtype=tf.float32)\n",
    "            )\n",
    "            \n",
    "            self.strides.append(strides[h])\n",
    "            \n",
    "            \n",
    "        for i in range(self.cDepth+1):\n",
    "            Codims1 = np.floor((Codims1 - kernelsshp[i][0])/(strides[i][1])) + 1\n",
    "            Codims2 = np.floor((Codims2 - kernelsshp[i][1])/(strides[i][2])) + 1\n",
    "            \n",
    "        print(Codims1,Codims2,self.ctNodes - self.ciNodes)\n",
    "            \n",
    "        self.CoDims = int(Codims1*Codims2) + 1\n",
    "        self.pDims = self.CoDims\n",
    "        \n",
    "#         self.Z = tf.Variable(tf.random_normal([self.pDims, self.CoDims]), name='Z', dtype=tf.float32)\n",
    "        self.Z = tf.Variable(tf.random_normal([2,2]), name='Z', dtype=tf.float32) \n",
    "\n",
    "        self.W = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        self.convs = []\n",
    "        self.cnodeProb = []\n",
    "        self.nodeProb = []\n",
    "        self.scores = []\n",
    "    \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        sigmaI = tf.reshape(sigmaI, [1,1])\n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        \n",
    "        Ximg = tf.reshape(X, [-1,self.d1,self.d2,self.channels])\n",
    "        \n",
    "        self.convs = []\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__cnodeProb = [] # node probability list\n",
    "        self.__cnodeProb.append(1) # probability of x passing through root is 1.\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        convT = 0.1*tf.nn.leaky_relu(tf.nn.conv2d(Ximg,\n",
    "            self.kernelsT[0],\n",
    "            padding=\"VALID\",\n",
    "            strides = self.strides[0]), name = 'convT0')\n",
    "        \n",
    "        self.convs.append(convT)\n",
    "        \n",
    "        flatConv = tf.layers.Flatten()(convT)\n",
    "        b = tf.squeeze(flatConv.shape[1])\n",
    "        self.wts.append(tf.Variable(tf.random_normal([1, b]), name='wts' + str(0), dtype=tf.float32))\n",
    "        self.wts1.append(tf.Variable(tf.random_normal([1, b]), name='wts1' + str(0), dtype=tf.float32))\n",
    "        self.wts2.append(tf.Variable(tf.random_normal([1, b]), name='wts2' + str(0), dtype=tf.float32))\n",
    "        self.bs.append(tf.Variable(tf.random_normal([1, 1]), name='bs' + str(0), dtype=tf.float32))\n",
    "        \n",
    "        finalImg =  None\n",
    "        \n",
    "        \n",
    "        fscore_ = None\n",
    "        fX_ = None\n",
    "        self.__nodeProbs = []\n",
    "        \n",
    "        for i in range(1,self.ctNodes):\n",
    "#             print(convT.shape, len(self.__cnodeProb))\n",
    "            parent_id = int(np.ceil(i / 2.0) - 1.0)\n",
    "            \n",
    "            convTprev = self.convs[parent_id]\n",
    "            flatConvP = tf.layers.Flatten()(convTprev)\n",
    "            \n",
    "            \n",
    "            cscore = tf.multiply(sigmaI, tf.matmul(self.wts[parent_id], flatConvP, transpose_b = True) + self.bs[parent_id])# 1 x _\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            cprob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(cscore)),2.0) # : scalar 1 x_\n",
    "            cprob = self.__cnodeProb[parent_id] * cprob # : scalar 1 x _\n",
    "            \n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__cnodeProb.append(cprob)\n",
    "            \n",
    "            convT = 0.1*tf.nn.leaky_relu(tf.nn.conv2d(convTprev,\n",
    "                self.kernelsT[i],\n",
    "                padding=\"VALID\",\n",
    "                strides = self.strides[i]), name = 'convT' + str(i))\n",
    "            \n",
    "            self.convs.append(convT)\n",
    "#             print(self.convs[i].shape)\n",
    "            \n",
    "            \n",
    "            flatConv = tf.layers.Flatten()(convT)\n",
    "            b = tf.squeeze(flatConv.shape[1])\n",
    "            \n",
    "            self.wts.append(tf.Variable(tf.random_normal([1, b]), name='wts' + str(i), dtype=tf.float32))\n",
    "            self.wts1.append(tf.Variable(tf.random_normal([1, b]), name='wts1' + str(i), dtype=tf.float32))\n",
    "            self.wts2.append(tf.Variable(tf.random_normal([1, b]), name='wts2' + str(i), dtype=tf.float32))\n",
    "            self.bs.append(tf.Variable(tf.random_normal([1, 1]), name='bs' + str(i), dtype=tf.float32))\n",
    "            \n",
    "\n",
    "            \n",
    "            if(i+1 > self.ciNodes):\n",
    "                # projected output of convolutional layers....\n",
    "                \n",
    "                iinum = i - self.ciNodes\n",
    "                \n",
    "                a,b = flatConv.shape\n",
    "                onesmat = flatConv[:,0:1]*0 + 1\n",
    "\n",
    "                flat_imgs = tf.concat([flatConv, onesmat], axis = 1)\n",
    "                \n",
    "#                 print(self.Z[iinum,:,:].shape, flatConv.shape, flat_imgs.shape)\n",
    "                \n",
    "#                 X_ = tf.divide(tf.matmul(self.Z[iinum,:,:], flat_imgs, transpose_b=True),self.pDims) # dimensions are D^x_\n",
    "                X_ = tf.transpose(flat_imgs)#tf.matmul(self.Z, flat_imgs, transpose_b = True)\n",
    "#                 print(X_)\n",
    "                # For Root Node score...\n",
    "                tnodeProb = [] # node probability list\n",
    "                tnodeProb.append(cprob) # probability of x passing through root is 1.\n",
    "                W_ = self.W[iinum, 0:(self.nClasses),:]# first K trees root W params : KxD^\n",
    "                V_ = self.V[iinum, 0:(self.nClasses),:]# first K trees root V params : KxD^\n",
    "\n",
    "                # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "                score_ = tnodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx_\n",
    "                self.scores.append(flat_imgs)\n",
    "#                 print(score_)\n",
    "        #         print(score_.shape)\n",
    "\n",
    "                # Adding rest of the nodes scores...\n",
    "                for t in range(1, self.tNodes):\n",
    "                    # current node is i\n",
    "                    # W, V of K different trees for current node\n",
    "                    W_ = self.W[iinum,t * self.nClasses:((t + 1) * self.nClasses),:]# : KxD^\n",
    "                    V_ = self.V[iinum,t * self.nClasses:((t + 1) * self.nClasses),:]# : KxD^\n",
    "\n",
    "\n",
    "                    # i's parent node shared theta param reshaping to 1xD^\n",
    "                    T_ = tf.reshape(self.T[iinum,int(np.ceil(t / 2.0) - 1.0),:],[-1, self.pDims])# : 1xD^\n",
    "\n",
    "                    # Calculating probability that x should come to this node next given it is in parent node...\n",
    "                    prob = tf.divide((1 + ((-1)**(t + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x_\n",
    "\n",
    "                    # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "                    prob = tnodeProb[int(np.ceil(t / 2.0) - 1.0)] * prob # : scalar 1x_\n",
    "\n",
    "                    # adding prob to node prob list\n",
    "                    tnodeProb.append(prob)\n",
    "                    # New score addes to sum of scores...\n",
    "                    score_ += tnodeProb[t]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx_\n",
    "                \n",
    "                self.scores.append(score_)\n",
    "                self.__nodeProbs.append(tnodeProb[1:])\n",
    "                \n",
    "                if(fscore_ is None):\n",
    "                    fscore_ = score_\n",
    "                    fX_ = tf.matmul(T_, X_)*cprob\n",
    "                else:\n",
    "                    fscore_ = fscore_ + score_\n",
    "                    fX_ = fX_ + tf.matmul(T_, X_)*cprob\n",
    "            else:\n",
    "                pass\n",
    "                if(fscore_ is None):\n",
    "                    W_ = self.wts1[i]\n",
    "                    V_ = self.wts2[i]\n",
    "                    X_ = flatConv\n",
    "                    fscore_ = cprob*tf.multiply(tf.matmul(W_, X_, transpose_b = True), tf.tanh(self.sigma * tf.matmul(V_, X_, transpose_b = True))) # Kx_\n",
    "                    fX_ = cscore*cprob\n",
    "                else:\n",
    "                    W_ = self.wts1[i]\n",
    "                    V_ = self.wts2[i]\n",
    "                    X_ = flatConv\n",
    "                    fscore_ = fscore_ + cprob*tf.multiply(tf.matmul(W_, X_, transpose_b = True), tf.tanh(self.sigma * tf.matmul(V_, X_, transpose_b = True))) # Kx_\n",
    "                    fX_ = fX_ + cscore*cprob\n",
    "\n",
    "                \n",
    "        self.score = fscore_ #- tf.reduce_min(fscore_,axis=0) + 0.00000000000000000000000000001\n",
    "        self.X_ = fX_\n",
    "        self.nodeProb = tf.convert_to_tensor(self.__nodeProbs[:])\n",
    "        self.cnodeProb = tf.convert_to_tensor(self.__cnodeProb[1:])\n",
    "        self.layers = self.convs\n",
    "#         print(self.score,cprob.shape, T_.shape, X_.shape)\n",
    "        return self.score, self.X_\n",
    "                \n",
    "   \n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is 1xk\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        llen = self.tree.ciNodes\n",
    "        var = 0\n",
    "        for i in range(llen):\n",
    "            var = var +  self.lT * tf.square(tf.norm(self.tree.wts[i]))\n",
    "            var = var + self.lW * tf.square(tf.norm(self.tree.wts1[i]))\n",
    "            var = var + self.lV * tf.square(tf.norm(self.tree.wts2[i]))\n",
    "            \n",
    "        self.regLoss = self.regLoss + var\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, saver, filename,valsig):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = valsig # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainAccOld = 0.0\n",
    "            trainLoss = 0.0\n",
    "            trainBest = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                \n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if ((itersInPhase+1) % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "#                     Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(Xcapeval)))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) )))*valsig/30)\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > (1/2)*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            print(\"SigmaI :\",treeSigmaI)\n",
    "            \n",
    "            if(((trainAcc / numIters) - trainAccOld) < -0.1):\n",
    "#                 saver.restore(sess, filename)\n",
    "                self.lr = tf.math.minimum(self.lr/10, 0.00001)\n",
    "#                 self.tree.sigma = tf.math.minimum(self.tree.sigma/10, 0.001)\n",
    "                trainAccOld = trainAcc/numIters \n",
    "\n",
    "            else:\n",
    "                self.lr = tf.math.maximum(self.lr*10, 0.1)\n",
    "#                 self.tree.sigma = tf.math.maximum(self.tree.sigma*10, 1)\n",
    "                trainAccOld = trainAcc/numIters \n",
    "#                 saver.save(sess, filename)\n",
    "\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "            if((trainAcc / numIters) <= trainBest):\n",
    "                pass\n",
    "            else:\n",
    "                trainBest = trainAcc/numIters \n",
    "                saver.save(sess, filename + \"/model_best\")\n",
    "                \n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if(testLoss == np.nan or regTestLoss == np.nan):\n",
    "                print(\"getting nan\")\n",
    "                saver.restore(sess, filename)\n",
    "                self.lr = self.lr/10\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0 4.0 4\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "kernelsshp = [[4,4,3,2],[4,4,2,1],[3,3,1,1],[3,3,2,1],[3,3,1,1],[3,3,1,1],[3,3,1,1]]\n",
    "strides = [[1,2,2,1],[1,2,2,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]]\n",
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 28, tDepth = 2, sigma = 1,\n",
    "              kernelsshp = kernelsshp, strides = strides , cDepth = 2)\n",
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.1, lT = 0.1, lV = 0.1, lZ = 0.00, lr = 0.001, X = X, Y = Y,\n",
    "                              sZ = 0.995, sW = 0.995, sV = 0.995, sT = 0.995)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "Train Loss: 640.2631574358259 Train accuracy: 0.10162857068436486\n",
      "SigmaI : 1\n",
      "Test accuracy 0.10472\n",
      "MarginLoss + RegLoss: 3.076416 + 633.5638 = 636.6402\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 633.1539481026786 Train accuracy: 0.10162857174873352\n",
      "SigmaI : 1\n",
      "Test accuracy 0.1088\n",
      "MarginLoss + RegLoss: 3.034546 + 626.5118 = 629.5463\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 626.120361328125 Train accuracy: 0.10162857174873352\n",
      "SigmaI : 1\n",
      "Test accuracy 0.10288\n",
      "MarginLoss + RegLoss: 3.0992432 + 619.53577 = 622.635\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 619.1657453264509 Train accuracy: 0.10194285746131625\n",
      "SigmaI : 1\n",
      "Test accuracy 0.1036\n",
      "MarginLoss + RegLoss: 3.069641 + 612.6392 = 615.70886\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 612.291765485491 Train accuracy: 0.1023428567818233\n",
      "SigmaI : 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-af63d99e4398>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mbonsaiTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalEpochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done sequence \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-36541993ae4f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, saver, filename, valsig)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0;31m# calculating losses....\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mtestAcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregTestLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestLoss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mregTestLoss\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filename = \"./bonsaiconvdiff/Nov17_1_19pm/model\"\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "#     saver.restore(sess, filename+'19')\n",
    "    sess.run(init_op)\n",
    "    saver.save(sess, filename+'last')\n",
    "    totalEpochs = 100\n",
    "    bonsaiTrainer.lr = 0.1\n",
    "    bonsaiTrainer.lZ = 0.0\n",
    "    bonsaiTrainer.tree.sigma = 0.00001\n",
    "    batchSize = np.maximum(5000, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "    for i in range(20):\n",
    "        bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest, saver, filename,1)\n",
    "        saver.save(sess, filename+str(i))\n",
    "        print(\"Done sequence \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    zs = np.sum(np.abs(tree.Z.eval())<0.000000000000001)/(tree.Z.eval()!=None).sum()\n",
    "    ws = np.sum(np.abs(tree.W.eval())<0.000000000000001)/(tree.W.eval()!=None).sum()\n",
    "    vs = np.sum(np.abs(tree.V.eval())<0.000000000000001)/(tree.V.eval()!=None).sum()\n",
    "    ts = np.sum(np.abs(tree.T.eval())<0.000000000000001)/(tree.T.eval()!=None).sum()\n",
    "    print('Sparse ratios achieved...\\nW:',1-ws,'\\nV:',1-vs,'\\nT:',1-ts,'\\nZ:',1-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction...\n",
    "sub = pd.read_csv('./cifartest/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarr = sub.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 2)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37500, 3072)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = img.imread('./cifartest/cifartest/test/1234.png')\n",
    "\n",
    "iii = 780\n",
    "image = np.flip(np.rollaxis(image.reshape(32,32,3),1,0),axis = 0)\n",
    "# image = Xtrain[1000].reshape(32,32,3)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=[1,1])\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([87, 85, 73, ..., 65, 59, 66], dtype=uint8),\n",
       " array([ 87,  89, 117, ..., 148, 142, 111]))"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain[iii], (image.reshape(-1,3072)[0]*255).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bonsaiconvdiff/Nov13_8_16pm/model0\n",
      "Sparse ratios achieved...\n",
      "W: 1.0 \n",
      "V: 1.0 \n",
      "T: 1.0 \n",
      "Z: 1.0\n",
      "[9]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, filename+'0')\n",
    "    calc_zero_ratios(tree)\n",
    "    _feed_dict = {bonsaiTrainer.X:(image*255).astype(int).reshape(-1,3072),bonsaiTrainer.sigmaI:float(1000.0)}\n",
    "    val = sess.run(tree.prediction, feed_dict=_feed_dict)\n",
    "    \n",
    "#     for item in val:\n",
    "#         print(item.shape)\n",
    "\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-320-c41f8ca80d77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 72x72 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = [1,1])\n",
    "plt.imshow(val[0][2][:,:,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_62:0' shape=(10, ?) dtype=float32>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "labellist = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bonsaiconvdiff/Nov12_9_36pm/model0\n",
      "the total parts 0 out of 300000 done\n",
      "the total parts 10000 out of 300000 done\n",
      "the total parts 20000 out of 300000 done\n",
      "the total parts 30000 out of 300000 done\n",
      "the total parts 40000 out of 300000 done\n",
      "the total parts 50000 out of 300000 done\n",
      "the total parts 60000 out of 300000 done\n",
      "the total parts 70000 out of 300000 done\n",
      "the total parts 80000 out of 300000 done\n",
      "the total parts 90000 out of 300000 done\n",
      "the total parts 100000 out of 300000 done\n",
      "the total parts 110000 out of 300000 done\n",
      "the total parts 120000 out of 300000 done\n",
      "the total parts 130000 out of 300000 done\n",
      "the total parts 140000 out of 300000 done\n",
      "the total parts 150000 out of 300000 done\n",
      "the total parts 160000 out of 300000 done\n",
      "the total parts 170000 out of 300000 done\n",
      "the total parts 180000 out of 300000 done\n",
      "the total parts 190000 out of 300000 done\n",
      "the total parts 200000 out of 300000 done\n",
      "the total parts 210000 out of 300000 done\n",
      "the total parts 220000 out of 300000 done\n",
      "the total parts 230000 out of 300000 done\n",
      "the total parts 240000 out of 300000 done\n",
      "the total parts 250000 out of 300000 done\n",
      "the total parts 260000 out of 300000 done\n",
      "the total parts 270000 out of 300000 done\n",
      "the total parts 280000 out of 300000 done\n",
      "the total parts 290000 out of 300000 done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, filename+'0')\n",
    "\n",
    "    for i in range(300000):\n",
    "        image = img.imread('./cifartest/cifartest/test/'+str(i+1)+'.png')\n",
    "        image = np.rollaxis(image,1,0)\n",
    "        _feed_dict = {bonsaiTrainer.X:(image*255).astype(int).reshape(-1,3072),bonsaiTrainer.sigmaI:float(1)}\n",
    "        val = sess.run(tree.prediction, feed_dict=_feed_dict)[0]\n",
    "        labelobt = labellist[val]\n",
    "        subarr[i,1] = labelobt\n",
    "        if((i)%10000 == 0):\n",
    "            print('the total parts',i, 'out of 300000 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm = pd.DataFrame(subarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  1     horse\n",
       "1  2  airplane\n",
       "2  3       dog\n",
       "3  4  airplane\n",
       "4  5      ship"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm = new_subm.rename(columns={0: 'id', 1: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['horse', 'airplane', 'dog', 'ship', 'cat', 'truck', 'automobile',\n",
       "       'deer', 'frog', 'bird'], dtype=object)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>horse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id     label\n",
       "0  1     horse\n",
       "1  2  airplane\n",
       "2  3       dog\n",
       "3  4  airplane\n",
       "4  5      ship"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm.to_csv('submission_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
