{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "source": [
    "# CIFAR - 10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = './cifar10/'\n",
    "Xtrain = np.load(dirc + 'Xtrain.npy').reshape(-1,32*32*3)\n",
    "Ytrain = np.load(dirc + 'Ytrain.npy')\n",
    "Xtest = np.load(dirc + 'Xtest.npy').reshape(-1,32*32*3)\n",
    "Ytest = np.load(dirc + 'Ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 50000 ,Data Dims: 3072 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 480 # index of data object....\n",
    "obj = Xtrain[i].reshape(32,32,3)\n",
    "plt.imshow(np.rollaxis(obj,1,0),cmap='gray')\n",
    "plt.show()\n",
    "print(Ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, kernels, strides, cDepth = 2, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.cDepth = cDepth\n",
    "        self.ciNodes = 2**self.cDepth - 1\n",
    "        self.ctNodes = 2*self.ciNodes + 1\n",
    "        \n",
    "        \n",
    "        self.kernelsT = []\n",
    "        \n",
    "        self.strides = []\n",
    "        \n",
    "        self.channels = 3\n",
    "        self.d1 = 32\n",
    "        self.d2 = 32\n",
    "        d1 = self.d1\n",
    "        d2 = self.d2\n",
    "        oD1 = d1\n",
    "        oD2 = d2\n",
    "        \n",
    "        \n",
    "        self.kernelsshp = kernels\n",
    "        self.strides = strides\n",
    "        self.wts = []\n",
    "        \n",
    "        \n",
    "        \n",
    "        h = 0\n",
    "        h_old = 0\n",
    "        Codims1 = self.d1\n",
    "        Codims2 = self.d2\n",
    "            \n",
    "            \n",
    "            \n",
    "        for i in range(self.cDepth):\n",
    "            Codims1 = np.floor((Codims1 - self.kernelsshp[i][0])/(self.strides[i][1])) + 1\n",
    "            Codims2 = np.floor((Codims2 - self.kernelsshp[i][1])/(self.strides[i][2])) + 1\n",
    "            \n",
    "        print(Codims1,Codims2,self.ctNodes - self.ciNodes)\n",
    "            \n",
    "#         self.CoDims = int(Codims1*Codims2*(self.ctNodes - self.ciNodes)) + 1\n",
    "        self.CoDims = int(Codims1*Codims2) + 1\n",
    "        self.pDims =self.CoDims\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.CoDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "#         self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        self.convs = []\n",
    "    \n",
    "    \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        \n",
    "        Ximg = tf.reshape(X, [-1,self.d1,self.d2,self.channels])\n",
    "        \n",
    "        self.convs = []\n",
    "        \n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        # Single conv part....\n",
    "        convT = Ximg\n",
    "        for h in range(self.cDepth):\n",
    "            curr_kernel = tf.get_variable('kernelT' + str(0) + \"_\" + str(h), self.kernelsshp[h], \n",
    "                             initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32),\n",
    "                             dtype=tf.float32)\n",
    "            \n",
    "            curr_stride = self.strides[h]\n",
    "            \n",
    "            convT = tf.nn.leaky_relu(tf.nn.conv2d(convT,\n",
    "                curr_kernel,\n",
    "                padding=\"VALID\",\n",
    "                strides = curr_stride), name = 'convT' + str(0) + \"_\" + str(h))\n",
    "        \n",
    "        flatConv = tf.layers.Flatten()(convT)\n",
    "        onesmat = flatConv[:,0:1]*0 + 1\n",
    "        flatConv = tf.concat([flatConv, onesmat], axis = 1)\n",
    "        \n",
    "        print('Hello ',flatConv.shape, convT.shape,' bye!')\n",
    "        self.convs.append(flatConv)\n",
    "        \n",
    "        X_ = tf.transpose(flatConv)#tf.divide(tf.matmul(self.Z, flatConv, transpose_b=True),self.pDims) # dimensions are D^x_\n",
    "        self.X_ = X_\n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx_\n",
    "#         print(score_.shape)\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            \n",
    "            convT = Ximg\n",
    "            for h in range(self.cDepth):\n",
    "                curr_kernel = tf.get_variable('kernelT' + str(i) + \"_\" + str(h), self.kernelsshp[h], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32),\n",
    "                                 dtype=tf.float32)\n",
    "\n",
    "                curr_stride = self.strides[h]\n",
    "\n",
    "                convT = tf.nn.leaky_relu(tf.nn.conv2d(convT,\n",
    "                    curr_kernel,\n",
    "                    padding=\"VALID\",\n",
    "                    strides = curr_stride), name = 'convT' + str(i) + \"_\" + str(h))\n",
    "\n",
    "            flatConv = tf.layers.Flatten()(convT)\n",
    "            onesmat = flatConv[:,0:1]*0 + 1\n",
    "            flatConv = tf.concat([flatConv, onesmat], axis = 1)\n",
    "\n",
    "#             print('Hello ',flatConv.shape, convT.shape,' bye!')\n",
    "            self.convs.append(flatConv)\n",
    "\n",
    "            X_ = tf.transpose(flatConv)#f.divide(tf.matmul(self.Z, flatConv, transpose_b=True),self.pDims) # dimensions are D^x_\n",
    "\n",
    "            self.X_ += X_ \n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x_\n",
    "            \n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x_\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx_\n",
    "            \n",
    "#         print(score_.shape)\n",
    "            \n",
    "        self.score = score_ #+ convScore_\n",
    "        self.X_ = tf.divide(self.X_, self.tNodes)\n",
    "        return self.score, self.X_\n",
    "        \n",
    "        \n",
    "        \n",
    "    def conv_score(self, conv):\n",
    "        \n",
    "        return tf.reduce_mean(tf.layers.Flatten()(conv), axis = 1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, saver, filename):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = 1 # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainAccOld = 0.0\n",
    "            trainLoss = 0.0\n",
    "            trainBest = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                \n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if (itersInPhase % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "                    Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (100 * self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 0.1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > (1/2)*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            if(((trainAcc / numIters) - trainAccOld) < -0.1):\n",
    "                saver.restore(sess, filename)\n",
    "                self.lr = tf.math.minimum(self.lr/10, 0.00001)\n",
    "                self.tree.sigma = tf.math.minimum(self.tree.sigma/10, 0.001)\n",
    "            else:\n",
    "                self.lr = tf.math.maximum(self.lr*10, 0.1)\n",
    "                self.tree.sigma = tf.math.maximum(self.tree.sigma*10, 1)\n",
    "                trainAccOld = trainAcc/numIters \n",
    "                saver.save(sess, filename)\n",
    "                \n",
    "        \n",
    "        \n",
    "            if((trainAcc / numIters) <= trainBest):\n",
    "                pass\n",
    "            else:\n",
    "                trainBest = trainAcc/numIters \n",
    "                saver.save(sess, filename + \"/model_best\")\n",
    "                \n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 3.0 4\n",
      "Hello  (?, 10) (?, 3, 3, 1)  bye!\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.name_scope('hidden') as scope:\n",
    "    kernels = [[5,5,3,1],[3,3,1,1],[3,3,1,1],[2,2,1,1],[3,3,1,1]]\n",
    "    strides = [[1,3,3,1],[1,3,3,1],[1,1,1,1],[1,2,2,1],[1,1,1,1]]\n",
    "    tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 37, tDepth = 3, sigma = 1, kernels = kernels, \n",
    "                  strides = strides, cDepth = 2)\n",
    "    X = tf.placeholder(\"float32\", [None, dDims])\n",
    "    Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "    bonsaiTrainer = BonsaiTrainer(tree, lW = 0.01, lT = 0.01, lV = 0.01, lZ = 0.001, lr = 0.01, X = X, Y = Y,\n",
    "                                  sZ = 0.999, sW = 0.999, sV = 0.999, sT = 0.999)\n",
    "    init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "Train Loss: 14.019137506484986 Train accuracy: 0.14586000084877015\n",
      "Test accuracy 0.1328\n",
      "MarginLoss + RegLoss: 2.7058334 + 6.3438444 = 9.049678\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 6.421434516906738 Train accuracy: 0.25477999985218047\n",
      "Test accuracy 0.1605\n",
      "MarginLoss + RegLoss: 3.721131 + 2.88563 = 6.606761\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 4.01375289440155 Train accuracy: 0.32200000047683713\n",
      "Test accuracy 0.1472\n",
      "MarginLoss + RegLoss: 3.1013532 + 1.494204 = 4.595557\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 2.9838059067726137 Train accuracy: 0.35327999979257585\n",
      "Test accuracy 0.1495\n",
      "MarginLoss + RegLoss: 3.7456203 + 0.87499183 = 4.620612\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 2.4892672634124757 Train accuracy: 0.3740199998021126\n",
      "Test accuracy 0.2027\n",
      "MarginLoss + RegLoss: 4.457743 + 0.56566447 = 5.0234075\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: 2.2179437470436096 Train accuracy: 0.38882000207901\n",
      "Test accuracy 0.2012\n",
      "MarginLoss + RegLoss: 3.7116342 + 0.39997283 = 4.111607\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: 2.124112306833267 Train accuracy: 0.37655999973416326\n",
      "Test accuracy 0.2531\n",
      "MarginLoss + RegLoss: 2.8549855 + 0.30134052 = 3.156326\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: 1.948049545288086 Train accuracy: 0.4073400008678436\n",
      "Test accuracy 0.2433\n",
      "MarginLoss + RegLoss: 3.1584423 + 0.23886293 = 3.3973053\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: 1.8677260303497314 Train accuracy: 0.4185200008749962\n",
      "Test accuracy 0.2702\n",
      "MarginLoss + RegLoss: 2.9045691 + 0.19640069 = 3.1009698\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: 1.8110256373882294 Train accuracy: 0.42713999897241595\n",
      "Test accuracy 0.2852\n",
      "MarginLoss + RegLoss: 2.877551 + 0.16708091 = 3.044632\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: 2.0434729433059693 Train accuracy: 0.3275200013816357\n",
      "Test accuracy 0.2453\n",
      "MarginLoss + RegLoss: 2.3315754 + 0.14491877 = 2.476494\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: 1.8143359124660492 Train accuracy: 0.40461999744176863\n",
      "Test accuracy 0.3129\n",
      "MarginLoss + RegLoss: 2.16635 + 0.1302031 = 2.296553\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: 1.7472497117519379 Train accuracy: 0.42530000120401384\n",
      "Test accuracy 0.3165\n",
      "MarginLoss + RegLoss: 2.2445219 + 0.120029576 = 2.3645515\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: 1.7318952250480653 Train accuracy: 0.43136000096797944\n",
      "Test accuracy 0.3149\n",
      "MarginLoss + RegLoss: 2.2329397 + 0.114472955 = 2.3474126\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: 1.7022146201133728 Train accuracy: 0.4379400008916855\n",
      "Test accuracy 0.3221\n",
      "MarginLoss + RegLoss: 2.1638386 + 0.10974984 = 2.2735884\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: 1.7190557312965393 Train accuracy: 0.4279199996590614\n",
      "Test accuracy 0.3254\n",
      "MarginLoss + RegLoss: 1.9584514 + 0.10315048 = 2.0616019\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: 1.6748243415355681 Train accuracy: 0.44358000010251997\n",
      "Test accuracy 0.3331\n",
      "MarginLoss + RegLoss: 1.8944921 + 0.09933535 = 1.9938275\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: 1.6620119404792786 Train accuracy: 0.44744000166654585\n",
      "Test accuracy 0.3375\n",
      "MarginLoss + RegLoss: 1.8607621 + 0.09545086 = 1.956213\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: 1.650048555135727 Train accuracy: 0.4503999972343445\n",
      "Test accuracy 0.3291\n",
      "MarginLoss + RegLoss: 1.8836195 + 0.09062181 = 1.9742414\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: 1.6618969857692718 Train accuracy: 0.4456800013780594\n",
      "Test accuracy 0.3403\n",
      "MarginLoss + RegLoss: 1.9008611 + 0.089426026 = 1.9902872\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: 1.74745725274086 Train accuracy: 0.4148199999332428\n",
      "Test accuracy 0.2827\n",
      "MarginLoss + RegLoss: 2.007136 + 0.08041441 = 2.0875504\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: 1.7303399455547332 Train accuracy: 0.4177400007843971\n",
      "Test accuracy 0.3163\n",
      "MarginLoss + RegLoss: 1.8687078 + 0.074578315 = 1.9432861\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: 1.717079029083252 Train accuracy: 0.41872000008821486\n",
      "Test accuracy 0.2944\n",
      "MarginLoss + RegLoss: 2.0059118 + 0.07016815 = 2.07608\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: 1.6907738864421844 Train accuracy: 0.4254200005531311\n",
      "Test accuracy 0.3171\n",
      "MarginLoss + RegLoss: 1.9329437 + 0.06704737 = 1.999991\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: 1.7177759206295014 Train accuracy: 0.4143999996781349\n",
      "Test accuracy 0.301\n",
      "MarginLoss + RegLoss: 1.9447562 + 0.06761824 = 2.0123744\n",
      "\n",
      "Epoch Number: 25\n"
     ]
    }
   ],
   "source": [
    "filename = \"./bonsaiconvsame/Nov2_1_49/model\"\n",
    "with tf.name_scope('hidden') as scope:\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "#         saver.restore(sess, filename + '/model_best')\n",
    "        sess.run(init_op)\n",
    "        totalEpochs = 100\n",
    "        bonsaiTrainer.lr = 0.01\n",
    "        bonsaiTrainer.tree.sigma = 0.001\n",
    "        batchSize = np.maximum(500, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "        for i in range(10):\n",
    "            bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest, saver, filename)\n",
    "            saver.save(sess, filename)\n",
    "            print(\"Done sequence \", i)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    zs = np.sum(np.abs(tree.Z.eval())<0.000000000000001)/(tree.Z.eval()!=None).sum()\n",
    "    ws = np.sum(np.abs(tree.W.eval())<0.000000000000001)/(tree.W.eval()!=None).sum()\n",
    "    vs = np.sum(np.abs(tree.V.eval())<0.000000000000001)/(tree.V.eval()!=None).sum()\n",
    "    ts = np.sum(np.abs(tree.T.eval())<0.000000000000001)/(tree.T.eval()!=None).sum()\n",
    "    print('Sparse ratios achieved...\\nW:',1-ws,'\\nV:',1-vs,'\\nT:',1-ts,'\\nZ:',1-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction...\n",
    "sub = pd.read_csv('./cifartest/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "subarr = sub.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subarr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = img.imread('./cifartest/cifartest/test/1.png')\n",
    "image = np.rollaxis(image,1,0)\n",
    "# image = Xtrain[0].reshape(32,32,3)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 129.6x129.6 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=[1.8,1.8])\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ytrain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bonsaiconvdiff/9_39/model/model_best\n",
      "Sparse ratios achieved...\n",
      "W: 0.335952380952381 \n",
      "V: 0.3371428571428572 \n",
      "T: 0.7142857142857143 \n",
      "Z: 1.0\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, filename+'/model_best')\n",
    "    calc_zero_ratios(tree)\n",
    "    _feed_dict = {bonsaiTrainer.X:image.reshape(-1,3072),bonsaiTrainer.sigmaI:float(10e9)}\n",
    "    val = sess.run(tree.prediction, feed_dict=_feed_dict)[0]\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "labellist = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bonsaiconvdiff/3_16/model/model_best\n",
      "the total parts 0 out of 300000 done\n",
      "the total parts 10000 out of 300000 done\n",
      "the total parts 20000 out of 300000 done\n",
      "the total parts 30000 out of 300000 done\n",
      "the total parts 40000 out of 300000 done\n",
      "the total parts 50000 out of 300000 done\n",
      "the total parts 60000 out of 300000 done\n",
      "the total parts 70000 out of 300000 done\n",
      "the total parts 80000 out of 300000 done\n",
      "the total parts 90000 out of 300000 done\n",
      "the total parts 100000 out of 300000 done\n",
      "the total parts 110000 out of 300000 done\n",
      "the total parts 120000 out of 300000 done\n",
      "the total parts 130000 out of 300000 done\n",
      "the total parts 140000 out of 300000 done\n",
      "the total parts 150000 out of 300000 done\n",
      "the total parts 160000 out of 300000 done\n",
      "the total parts 170000 out of 300000 done\n",
      "the total parts 180000 out of 300000 done\n",
      "the total parts 190000 out of 300000 done\n",
      "the total parts 200000 out of 300000 done\n",
      "the total parts 210000 out of 300000 done\n",
      "the total parts 220000 out of 300000 done\n",
      "the total parts 230000 out of 300000 done\n",
      "the total parts 240000 out of 300000 done\n",
      "the total parts 250000 out of 300000 done\n",
      "the total parts 260000 out of 300000 done\n",
      "the total parts 270000 out of 300000 done\n",
      "the total parts 280000 out of 300000 done\n",
      "the total parts 290000 out of 300000 done\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, filename+'/model_best')\n",
    "\n",
    "    for i in range(300000):\n",
    "        image = img.imread('./cifartest/cifartest/test/'+str(i+1)+'.png')\n",
    "        image = np.rollaxis(image,1,0)\n",
    "        _feed_dict = {bonsaiTrainer.X:image.reshape(-1,3072),bonsaiTrainer.sigmaI:float(10e9)}\n",
    "        val = sess.run(tree.prediction, feed_dict=_feed_dict)[0]\n",
    "        labelobt = labellist[val]\n",
    "        subarr[i,1] = labelobt\n",
    "        if((i)%10000 == 0):\n",
    "            print('the total parts',i, 'out of 300000 done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm = pd.DataFrame(subarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0         1\n",
       "0  1      ship\n",
       "1  2  airplane\n",
       "2  3      frog\n",
       "3  4  airplane\n",
       "4  5  airplane"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm = new_subm.rename(columns={0: 'id', 1: 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ship', 'airplane', 'frog', 'dog', 'deer', 'bird', 'truck',\n",
       "       'horse', 'automobile', 'cat'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>airplane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id     label\n",
       "0  1      ship\n",
       "1  2  airplane\n",
       "2  3      frog\n",
       "3  4  airplane\n",
       "4  5  airplane"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_subm.to_csv('submission_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144.536875"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
