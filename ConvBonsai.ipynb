{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "dirc = './Datasets/mnist_small/'\n",
    "Xtrain = np.load(dirc + 'Xtrain.npy')\n",
    "Ytrain = np.load(dirc + 'Ytrain.npy')\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = tts(Xtrain, Ytrain, stratify = Ytrain, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder as LE \n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "mo1 = LE()\n",
    "mo2 = OHE()\n",
    "\n",
    "Ytrain = mo2.fit_transform(mo1.fit_transform((Ytrain.ravel())).reshape(-1,1)).todense()\n",
    "\n",
    "Ytest = mo2.transform(mo1.transform((Ytest.ravel())).reshape(-1,1)).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "Xtrain.shape, Xtest.shape,Ytrain.shape,Ytest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('convbonsai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, kernelsshp, strides, cDepth = 2, W=None, T=None, V=None, Z=None, ch = None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.cDepth = cDepth\n",
    "        self.ciNodes = 2**self.cDepth - 1\n",
    "        self.ctNodes = 2*self.ciNodes + 1\n",
    "        \n",
    "        \n",
    "        self.kernelsT = []\n",
    "        \n",
    "        self.strides = []\n",
    "        \n",
    "        if(ch is None):\n",
    "            ch = 3\n",
    "        \n",
    "        self.channels = ch    \n",
    "        var = int(np.sqrt(self.dDims/self.channels))\n",
    "        self.d1 = var\n",
    "        self.d2 = var\n",
    "        d1 = self.d1\n",
    "        d2 = self.d2\n",
    "        \n",
    "        assert d1*d2*ch == self.dDims, \" Dimension mismatch, doesn't seem like it's a image or set channel(ch) = 1\"\n",
    "        \n",
    "        oD1 = d1\n",
    "        oD2 = d2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.wts = []\n",
    "        self.wts1 = []\n",
    "        self.wts2 = []\n",
    "        self.bs = []\n",
    "        \n",
    "        \n",
    "        h = 0\n",
    "        h_old = 0\n",
    "        Codims1 = self.d1\n",
    "        Codims2 = self.d2\n",
    "        \n",
    "        with tf.name_scope(\"Params\"):\n",
    "            for i in range(self.ctNodes):\n",
    "\n",
    "                h = int(np.floor(np.log(i+1)/np.log(2)))\n",
    "\n",
    "                self.kernelsT.append(\n",
    "                    tf.get_variable('kernelT'+str(i), kernelsshp[h], \n",
    "                                 initializer=tf.truncated_normal_initializer(stddev=5e-2, dtype=tf.float32),\n",
    "                                 dtype=tf.float32)\n",
    "                )\n",
    "\n",
    "                self.strides.append(strides[h])\n",
    "\n",
    "\n",
    "            for i in range(self.cDepth+1):\n",
    "                Codims1 = np.floor((Codims1 - kernelsshp[i][0])/(strides[i][1])) + 1\n",
    "                Codims2 = np.floor((Codims2 - kernelsshp[i][1])/(strides[i][2])) + 1\n",
    "\n",
    "            print(Codims1,Codims2,self.ctNodes - self.ciNodes)\n",
    "\n",
    "            self.CoDims = int(Codims1*Codims2) + 1\n",
    "            self.pDims = self.CoDims\n",
    "\n",
    "    #         self.Z = tf.Variable(tf.random_normal([self.pDims, self.CoDims]), name='Z', dtype=tf.float32)\n",
    "            self.Z = tf.Variable(tf.random_normal([2,2]), name='Z', dtype=tf.float32) \n",
    "\n",
    "            self.W = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "            self.V = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "            self.T = tf.Variable(tf.random_normal([self.ctNodes - self.ciNodes, self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        self.convs = []\n",
    "        self.cnodeProb = []\n",
    "        self.nodeProb = []\n",
    "        self.scores = []\n",
    "    \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        sigmaI = tf.reshape(sigmaI, [1,1])\n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        \n",
    "        Ximg = tf.reshape(X, [-1,self.d1,self.d2,self.channels])\n",
    "        \n",
    "        self.convs = []\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__cnodeProb = [] # node probability list\n",
    "        self.__cnodeProb.append(1) # probability of x passing through root is 1.\n",
    "        \n",
    "        with tf.name_scope('ConvNode'+str(0)):\n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "            convT = 0.1*tf.nn.leaky_relu(tf.nn.conv2d(Ximg,\n",
    "                self.kernelsT[0],\n",
    "                padding=\"VALID\",\n",
    "                strides = self.strides[0]), name = 'convT0')\n",
    "\n",
    "            self.convs.append(convT)\n",
    "\n",
    "            flatConv = tf.layers.Flatten()(convT)\n",
    "            b = tf.squeeze(flatConv.shape[1])\n",
    "            self.wts.append(tf.Variable(tf.random_normal([1, b]), name='wts' + str(0), dtype=tf.float32))\n",
    "            self.bs.append(tf.Variable(tf.random_normal([1, 1]), name='bs' + str(0), dtype=tf.float32))\n",
    "\n",
    "            finalImg =  None\n",
    "\n",
    "\n",
    "            fscore_ = None\n",
    "            fX_ = None\n",
    "            self.__nodeProbs = []\n",
    "        \n",
    "        for i in range(1,self.ctNodes):\n",
    "            with tf.name_scope('ConvNode'+str(i)):\n",
    "#             print(convT.shape, len(self.__cnodeProb))1\n",
    "                parent_id = int(np.ceil(i / 2.0) - 1.0)\n",
    "\n",
    "                convTprev = self.convs[parent_id]\n",
    "                flatConvP = tf.layers.Flatten()(convTprev)\n",
    "\n",
    "\n",
    "                cscore = tf.multiply(sigmaI, tf.matmul(self.wts[parent_id], flatConvP, transpose_b = True) + self.bs[parent_id])# 1 x _\n",
    "\n",
    "                # Calculating probability that x should come to this node next given it is in parent node...\n",
    "                cprob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(cscore)),2.0) # : scalar 1 x_\n",
    "                cprob = self.__cnodeProb[parent_id] * cprob # : scalar 1 x _\n",
    "\n",
    "\n",
    "                # adding prob to node prob list\n",
    "                self.__cnodeProb.append(cprob)\n",
    "\n",
    "                convT = 0.1*tf.nn.leaky_relu(tf.nn.conv2d(convTprev,\n",
    "                    self.kernelsT[i],\n",
    "                    padding=\"VALID\",\n",
    "                    strides = self.strides[i]), name = 'convT' + str(i))\n",
    "\n",
    "                self.convs.append(convT)\n",
    "    #             print(self.convs[i].shape)\n",
    "\n",
    "\n",
    "                flatConv = tf.layers.Flatten()(convT)\n",
    "                b = tf.squeeze(flatConv.shape[1])\n",
    "\n",
    "                self.wts.append(tf.Variable(tf.random_normal([1, b]), name='wts' + str(i), dtype=tf.float32))\n",
    "    #             self.wts1.append(tf.Variable(tf.random_normal([1, b]), name='wts1' + str(i), dtype=tf.float32))\n",
    "    #             self.wts2.append(tf.Variable(tf.random_normal([1, b]), name='wts2' + str(i), dtype=tf.float32))\n",
    "                self.bs.append(tf.Variable(tf.random_normal([1, 1]), name='bs' + str(i), dtype=tf.float32))\n",
    "            \n",
    "\n",
    "            \n",
    "            if(i+1 > self.ciNodes):\n",
    "                # projected output of convolutional layers....\n",
    "                \n",
    "                iinum = i - self.ciNodes\n",
    "                \n",
    "                a,b = flatConv.shape\n",
    "                onesmat = flatConv[:,0:1]*0 + 1\n",
    "\n",
    "                flat_imgs = tf.concat([flatConv, onesmat], axis = 1)\n",
    "                \n",
    "#                 print(self.Z[iinum,:,:].shape, flatConv.shape, flat_imgs.shape)\n",
    "                \n",
    "#                 X_ = tf.divide(tf.matmul(self.Z[iinum,:,:], flat_imgs, transpose_b=True),self.pDims) # dimensions are D^x_\n",
    "                X_ = tf.transpose(flat_imgs)#tf.matmul(self.Z, flat_imgs, transpose_b = True)\n",
    "#                 print(X_)\n",
    "                # For Root Node score...\n",
    "                tnodeProb = [] # node probability list\n",
    "                tnodeProb.append(cprob) # probability of x passing through root is 1.\n",
    "                W_ = self.W[iinum, 0:(self.nClasses),:]# first K trees root W params : KxD^\n",
    "                V_ = self.V[iinum, 0:(self.nClasses),:]# first K trees root V params : KxD^\n",
    "\n",
    "                # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "                score_ = tnodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx_\n",
    "                self.scores.append(flat_imgs)\n",
    "#                 print(score_)\n",
    "        #         print(score_.shape)\n",
    "\n",
    "                # Adding rest of the nodes scores...\n",
    "#                 with tf.name_scope('BonTree'+str(i)):\n",
    "                for t in range(1, self.tNodes):\n",
    "                    with tf.name_scope('BonNode'+str(i)+str(t)):\n",
    "                    # current node is i\n",
    "                    # W, V of K different trees for current node\n",
    "                        W_ = self.W[iinum,t * self.nClasses:((t + 1) * self.nClasses),:]# : KxD^\n",
    "                        V_ = self.V[iinum,t * self.nClasses:((t + 1) * self.nClasses),:]# : KxD^\n",
    "\n",
    "\n",
    "                        # i's parent node shared theta param reshaping to 1xD^\n",
    "                        T_ = tf.reshape(self.T[iinum,int(np.ceil(t / 2.0) - 1.0),:],[-1, self.pDims])# : 1xD^\n",
    "\n",
    "                        # Calculating probability that x should come to this node next given it is in parent node...\n",
    "                        prob = tf.divide((1 + ((-1)**(t + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x_\n",
    "\n",
    "                        # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "                        prob = tnodeProb[int(np.ceil(t / 2.0) - 1.0)] * prob # : scalar 1x_\n",
    "\n",
    "                        # adding prob to node prob list\n",
    "                        tnodeProb.append(prob)\n",
    "                        # New score addes to sum of scores...\n",
    "                        score_ += tnodeProb[t]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx_\n",
    "\n",
    "                self.scores.append(score_)\n",
    "                self.__nodeProbs.append(tnodeProb[1:])\n",
    "\n",
    "                if(fscore_ is None):\n",
    "                    fscore_ = score_\n",
    "                    fX_ = tf.matmul(T_, X_)*cprob\n",
    "                else:\n",
    "                    fscore_ = fscore_ + score_\n",
    "                    fX_ = fX_ + tf.matmul(T_, X_)*cprob\n",
    "            else:\n",
    "                pass\n",
    "#                 if(fscore_ is None):\n",
    "#                     W_ = self.wts1[i]\n",
    "#                     V_ = self.wts2[i]\n",
    "#                     X_ = flatConv\n",
    "#                     fscore_ = cprob*tf.multiply(tf.matmul(W_, X_, transpose_b = True), tf.tanh(self.sigma * tf.matmul(V_, X_, transpose_b = True))) # Kx_\n",
    "#                     fX_ = cscore*cprob\n",
    "#                 else:\n",
    "#                     W_ = self.wts1[i]\n",
    "#                     V_ = self.wts2[i]\n",
    "#                     X_ = flatConv\n",
    "#                     fscore_ = fscore_ + cprob*tf.multiply(tf.matmul(W_, X_, transpose_b = True), tf.tanh(self.sigma * tf.matmul(V_, X_, transpose_b = True))) # Kx_\n",
    "#                     fX_ = fX_ + cscore*cprob\n",
    "\n",
    "                \n",
    "        self.score = fscore_ #- tf.reduce_min(fscore_,axis=0) + 0.00000000000000000000000000001\n",
    "        self.X_ = fX_\n",
    "        self.nodeProb = tf.convert_to_tensor(self.__nodeProbs[:])\n",
    "        self.cnodeProb = tf.convert_to_tensor(self.__cnodeProb[1:])\n",
    "        self.layers = self.convs\n",
    "#         print(self.score,cprob.shape, T_.shape, X_.shape)\n",
    "        return self.score, self.X_\n",
    "                \n",
    "   \n",
    "        \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is 1xk\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        with tf.name_scope(\"IHT\"):\n",
    "            # place holders for sparse parameters....\n",
    "            self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "            self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "            self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "            self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "            # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "            self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "            self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "            self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "            self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "            # grouping the graph objects as one object....\n",
    "            self.hardThresholdGroup = tf.group(\n",
    "                self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        with tf.name_scope(\"ACC\"):\n",
    "            if (self.tree.nClasses > 2):\n",
    "                correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "            else:\n",
    "                # some accuracy functional analysis for 2 classes could be different from this...\n",
    "                y_ = self.Y * 2 - 1\n",
    "                correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "                correctPrediction = tf.nn.relu(correctPrediction)\n",
    "                correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "                self.accuracy = tf.reduce_mean(\n",
    "                    tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        with tf.name_scope(\"Loss\"):\n",
    "            # regularization losses.....\n",
    "            self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                              self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                              self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                              self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "\n",
    "            llen = self.tree.ciNodes\n",
    "            var = 0\n",
    "            for i in range(llen):\n",
    "                var = var +  self.lT * tf.square(tf.norm(self.tree.wts[i]))\n",
    "    #             var = var + self.lW * tf.square(tf.norm(self.tree.wts1[i]))\n",
    "    #             var = var + self.lV * tf.square(tf.norm(self.tree.wts2[i]))\n",
    "\n",
    "            self.regLoss = self.regLoss + var\n",
    "\n",
    "            # emperical actual loss.....\n",
    "            if (self.tree.nClasses > 2):\n",
    "                '''\n",
    "                Cross Entropy loss for MultiClass case in joint training for\n",
    "                faster convergence\n",
    "                '''\n",
    "                # cross entropy loss....\n",
    "                self.marginLoss = tf.reduce_mean(\n",
    "                    tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                                   labels=tf.stop_gradient(self.Y)))\n",
    "            else:\n",
    "                # sigmoid loss....\n",
    "                self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "\n",
    "            # adding the losses...\n",
    "            self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval, saver, filename,valsig):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = valsig # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainAccOld = 0.0\n",
    "            trainLoss = 0.0\n",
    "            trainBest = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                \n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if ((itersInPhase) % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "#                     Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(Xcapeval)))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) )))*valsig/30)\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = max(sum_tr, treeSigmaI)\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > (1/2)*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            print(\"SigmaI :\",treeSigmaI,\":LR:\",self.lr)\n",
    "            \n",
    "#             if(((trainAcc / numIters) - trainAccOld) < -0.1):\n",
    "# #                 saver.restore(sess, filename)\n",
    "#                 self.lr = tf.math.minimum(self.lr/10, 0.00001)\n",
    "# #                 self.tree.sigma = tf.math.minimum(self.tree.sigma/10, 0.001)\n",
    "#                 trainAccOld = trainAcc/numIters \n",
    "\n",
    "#             else:\n",
    "#                 self.lr = tf.math.maximum(self.lr*10, 0.1)\n",
    "# #                 self.tree.sigma = tf.math.maximum(self.tree.sigma*10, 1)\n",
    "#                 trainAccOld = trainAcc/numIters \n",
    "# #                 saver.save(sess, filename)\n",
    "\n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "#             if((trainAcc / numIters) <= trainBest):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 trainBest = trainAcc/numIters \n",
    "#                 saver.save(sess, filename + \"/model_best\")\n",
    "                \n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "#             if(testLoss == np.nan or regTestLoss == np.nan):\n",
    "#                 print(\"getting nan\")\n",
    "#                 saver.restore(sess, filename)\n",
    "#                 self.lr = self.lr/10\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "                saver.save(sess, filename + \"/model_best\")\n",
    "                \n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acc achieved : 60 % on test 93 % on train...\n",
    "# filename = \"./bonsaiconvdiff/Nov20_6_23pm/model\"\n",
    "# tf.reset_default_graph()\n",
    "# kernelsshp = [[4,4,3,30],[4,4,30,20],[3,3,20,1],[2,2,5,1],[3,3,1,1],[3,3,1,1],[3,3,1,1]]\n",
    "# strides = [[1,2,2,1],[1,2,2,1],[1,2,2,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]]\n",
    "# tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 28, tDepth = 3, sigma = 1,\n",
    "#               kernelsshp = kernelsshp, strides = strides , cDepth = 2)\n",
    "# X = tf.placeholder(\"float32\", [None, dDims])\n",
    "# Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "# bonsaiTrainer = BonsaiTrainer(tree, lW = 0.000001, lT = 0.000001, lV = 0.000001, lZ = 0.00, lr = 0.0001, X = X, Y = Y,\n",
    "#                               sZ = 0.995, sW = 0.995, sV = 0.995, sT = 0.995)\n",
    "# init_op = tf.global_variables_initializer()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "kernelsshp = [[4,4,1,3],[4,4,3,2],[3,3,2,1],[2,2,5,1],[3,3,1,1],[3,3,1,1],[3,3,1,1]]\n",
    "strides = [[1,2,2,1],[1,2,2,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1],[1,1,1,1]]\n",
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 28, tDepth = 2, sigma = 1,\n",
    "              kernelsshp = kernelsshp, strides = strides , cDepth = 2, ch = 1)\n",
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])\n",
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.000001, lT = 0.000001, lV = 0.000001, lZ = 0.00, lr = 0.01, X = X, Y = Y,\n",
    "                              sZ = 0.995, sW = 0.995, sV = 0.995, sT = 0.995)\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./bonsaiconvdiff/Nov23_6_23pm/model\"\n",
    "with tf.name_scope('hidden') as scope:\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.Saver()\n",
    "#         saver.restore(sess, filename+'2')\n",
    "        sess.run(init_op)\n",
    "        writer = tf.summary.FileWriter('convbonsai')\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        \n",
    "        saver.save(sess, filename)\n",
    "        totalEpochs = 10\n",
    "#         bonsaiTrainer.lr = 0.1\n",
    "#         bonsaiTrainer.lW = 0.000001\n",
    "#         bonsaiTrainer.lT = 0.000001\n",
    "#         bonsaiTrainer.lV = 0.000001\n",
    "#         bonsaiTrainer.tree.sigma = 0.1\n",
    "        batchSize = np.maximum(1000, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "        for i in range(5):\n",
    "            bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest, saver, filename,1)\n",
    "            saver.save(sess, filename + str(i))\n",
    "            print(\"Done sequence \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    zs = np.sum(np.abs(tree.Z.eval())>0.000000000000001)\n",
    "    ws = np.sum(np.abs(tree.W.eval())>0.000000000000001)\n",
    "    vs = np.sum(np.abs(tree.V.eval())>0.000000000000001)\n",
    "    ts = np.sum(np.abs(tree.T.eval())>0.000000000000001)\n",
    "    print('Sparse ratios achieved...\\nW:',ws,'\\nV:',vs,'\\nT:',ts,'\\nZ:',zs)\n",
    "    var = (ws+vs+ts)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Xtrain[:10000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, filename+'3')\n",
    "    var = calc_zero_ratios(tree)\n",
    "    _feed_dict = {bonsaiTrainer.X:(image*255).astype(int).reshape(-1,dDims),bonsaiTrainer.sigmaI:float(1)}\n",
    "    \n",
    "    start = time.time()\n",
    "    val = sess.run([tree.prediction, tree.wts, tree.bs, tree.convs, tree.kernelsT, tree.cnodeProb], feed_dict=_feed_dict)\n",
    "    end = time.time()\n",
    "    \n",
    "    size = 0\n",
    "    for i in range(len(val[1])):\n",
    "        size += np.sum(val[1][i]>0.0000000001)\n",
    "        size += np.sum(val[2][i]>0.0000000001)\n",
    "        \n",
    "#     for item in val:\n",
    "#         print(item.shape)\n",
    "\n",
    "    print('Number of non_zero paramters : ',var + size,' Time taken : ', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LL = []\n",
    "LR = []\n",
    "RR = []\n",
    "RL = []\n",
    "for i in range(len(val[0])):\n",
    "\n",
    "    if(np.round(val[-1][:,:,i])[0] == 1 and np.round(val[-1][:,:,i])[2] == 1):\n",
    "        LL.append(val[0][i])\n",
    "    elif(np.round(val[-1][:,:,i])[0] == 1 and np.round(val[-1][:,:,i])[3] == 1):\n",
    "        LR.append(val[0][i])\n",
    "    elif(np.round(val[-1][:,:,i])[1] == 1 and np.round(val[-1][:,:,i])[4] == 1):\n",
    "        RL.append(val[0][i])\n",
    "    elif(np.round(val[-1][:,:,i])[1] == 1 and np.round(val[-1][:,:,i])[5] == 1):\n",
    "        RR.append(val[0][i])\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(RL, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(LL, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(RR, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(LR, return_counts = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
