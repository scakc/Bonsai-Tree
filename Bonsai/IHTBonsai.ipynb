{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from IPython.display import display, clear_output\n",
    "import sys\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USPS Data\n",
    "\n",
    "Dataset has been already preprocessed and onehotted...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [],
   "source": [
    "#Loading Pre-processed dataset for Bonsai\n",
    "Xtrain = np.load('Xtrain.npy')\n",
    "Ytrain = np.load('Ytrain.npy')\n",
    "Xtest = np.load('Xtest.npy')\n",
    "Ytest = np.load('Ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T12:06:06.104645Z",
     "start_time": "2018-08-15T12:06:06.058368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 7291 ,Data Dims: 257 ,No. Classes: 10\n"
     ]
    }
   ],
   "source": [
    "# N, dDims = X_train.shape\n",
    "N, dDims = Xtrain.shape\n",
    "# nClasses = len(np.unique(Y_train))\n",
    "nClasses = Ytrain.shape[1]\n",
    "print('Training Size:',N,',Data Dims:', dDims,',No. Classes:', nClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEhdJREFUeJzt3X+QVeV9x/H3d3/wY2EREAQUKmCM1WitSB1jOjYp0SC1EieZFqdpaUwnzaS22jZNyDjTZPpXk7T50dZJxqqJaRnN1B/VZrSRMXHaNEpUCiiiggiKoCAoIL/217d/3IO5LLvsPt977mHp83nN7OzdvefZ59lzz+eec889z/2auyMi+Wk50QMQkRND4RfJlMIvkimFXyRTCr9IphR+kUwp/CKZUvhFMqXwi2SqrcrOWjvGefvEycntLHIRYvDCxb4x6Q3P7Hwz1NeE4FNvX+Cf6wpeydnlraF2B31Ucpu3u8eG+uo62J7cpqUr1FVsWwTcAm0C20f327vpPbB/WL1VGv72iZM584//Irld5IFq6UlvA/DOOemd3fKh74b6WjC2N9TusHcnt9nSE9vaN3dPDLV79tCs5Db/sf2CUF9bnp+e3Gb8y7Entdbgk0bPmPQ2vaPT22y+/evDXlaH/SKZaij8ZrbQzF4ws41mtqysQYlI84XDb2atwC3AVcB5wHVmdl5ZAxOR5mpkz38JsNHdN7l7F3A3sLicYYlIszUS/jOAV+t+3lr8TkROAo2Ef6C3E455I8TMPm1mT5nZU70H9jfQnYiUqZHwbwXq38+ZCWzrv5C73+ru8919fmvHuAa6E5EyNRL+J4GzzWyOmY0ClgAPljMsEWm28EU+7t5jZjcAPwJagTvcfV1pIxORpmroCj93fwh4qKSxiEiFdIWfSKYUfpFMVTqxB8ACE24ikyna9semX3U+lz4b7Y96rw/1NWHaO6F2o9vTV+Le/YGZJYAFZqMBXDDjmDd+hrRw+nOhvp4ck/4W8tr9Z4f6Grc1tkIiE8360icrJs061J5fJFMKv0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKYVfJFMKv0imFH6RTCn8IpmqdGKPObQEitS0pBeooT34cYGdW9NnYExd0xfqq/VQoCQL0LYzfabT1AO7Q331TTkl1G7donOS25z7u6+H+rpm6urkNk9PnR3qq297+sQvAAtsIpE2KbTnF8mUwi+SKYVfJFONlOuaZWY/MbP1ZrbOzG4sc2Ai0lyNnPDrAf7S3VeZWSfwtJmtcPfYx7GISKXCe3533+7uq4rb+4D1qFyXyEmjlNf8ZjYbuAhYOcB975br6lG5LpERo+Hwm9l44F7gJnff2//++nJdbSrXJTJiNBR+M2unFvzl7n5fOUMSkSo0crbfgNuB9e7+9fKGJCJVaGTP/wHg94HfNLPVxdeiksYlIk3WSKHOnwLBkg4icqLpCj+RTFVfrisyuylU4itWrmvUvvQphO3bj3mTY3h2vRVq1rcnvb/WqVNCfXVPjJX5Ojgt/YH+pVG7Qn29fPi05DYte2Kbfuvh2HblrekHySmltyK05xfJlMIvkimFXyRTCr9IphR+kUwp/CKZUvhFMqXwi2RK4RfJlMIvkimFXyRTCr9Ipqqd2ONAqGxR+gwHb03vB6BrQntym97Rk0J9+ZxYu55x6f/c3lmxFbL/koOhdt+85PvJbV7tPjXU1/eeuiy5zanrY7PR2w7GZtt0dQYaeWCMCcPTnl8kUwq/SKYUfpFMlfHR3a1m9r9m9sMyBiQi1Shjz38jtWo9InISafRz+2cCvwXcVs5wRKQqje75vwl8ntAbeCJyIjVStONqYIe7Pz3Ecu/W6us9qFp9IiNFo0U7rjGzzcDd1Ip3/Gv/hepr9bWOVa0+kZGikRLdX3T3me4+G1gC/NjdP1HayESkqfQ+v0imSrm2390fAx4r42+JSDW05xfJ1ElRritSDvTQ5Njz2q7z0zvrPi32TmfnlNi7HxdP35rc5kMTY9dhLRq3JdSu3dLX/2Vrrw31NfW/02diduwI1IADuiYEp4tGqFyXiDSDwi+SKYVfJFMKv0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKYVfJFMKv0imFH6RTFU/qy8wU8ktfabdoVjZN+Ze+kpym+tn/jTU13vbd4TaTWntTm6zrXd0qK9tvbFZbLt7O5LbdHfH+uoLNPPWWK2+SPm8WsP0JpGspNCeXyRTCr9Iphot2jHRzO4xs+fNbL2Zvb+sgYlIczX6mv9bwH+6+8fNbBSQ/kJPRE6IcPjNbAJwOfCHAO7eBXSVMywRabZGDvvnAjuB7xZVem8zM1XlEDlJNBL+NmAe8G13vwjYDyzrv9BR5boOqFyXyEjRSPi3AlvdfWXx8z3UngyOclS5rg4dGIiMFI2U63odeNXMzil+tQB4rpRRiUjTNXq2/0+B5cWZ/k3AJxsfkohUoaHwu/tqYH5JYxGRCukKP5FMVTuxx4lNcOhLbzRqT2wGxgsvnpHc5ltdC0J9Re3YPSG5Te+u2MQeHxMrRfa+s9NLin34rBdDfa3o++XkNt0rY9ejjdkdm20TmaQTKm2XQHt+kUwp/CKZUvhFMqXwi2RK4RfJlMIvkimFXyRTCr9IphR+kUwp/CKZUvhFMqXwi2RK4RfJVOXluiKz+lp609uMejM2JWrM/6TXfjr85LRQX4GqWwDM2JO+Qtr39YT6aumOrcft585ObrP3Y2NCff3VRY8kt/mqfyTUV8vjY0PtWg+nb/gW2O5TZg9qzy+SKYVfJFONluv6czNbZ2bPmtldZhY7bhORyoXDb2ZnAH8GzHf384FWYElZAxOR5mr0sL8NGGtmbdTq9G1rfEgiUoVGPrf/NeDvgFeA7cAed08/7SoiJ0Qjh/2TgMXAHOB0YJyZfWKA5X5RruugynWJjBSNHPZ/GHjZ3Xe6ezdwH3BZ/4WOKtc1VuW6REaKRsL/CnCpmXWYmVEr17W+nGGJSLM18pp/JbXinKuAZ4q/dWtJ4xKRJmu0XNeXgC+VNBYRqZCu8BPJlMIvkqlKZ/UZsRl6kdlNkVlUAO0H0jtr6Y71FZ0x17o/fTpg6zuHQ32x6+1Qs2lvnpLcZuOc6aG+Zp+1M7nNle+NnZt+7Ll5oXYdb6S3Ua0+EWkKhV8kUwq/SKYUfpFMKfwimVL4RTKl8ItkSuEXyZTCL5IphV8kUwq/SKYUfpFMVVuuy8F6qylbFJlABITKiXmLhbrqa48997a0Vfic3RtckW+mTwgav2VKqKsNXekTgs7t2B7q69GxsUlcke2KyMQelesSkaEo/CKZGjL8ZnaHme0ws2frfjfZzFaY2Ybi+6TmDlNEyjacPf/3gIX9frcMeNTdzwYeLX4WkZPIkOF39/8Cdvf79WLgzuL2ncBHSx6XiDRZ9DX/NHffDlB8P628IYlIFZp+wq++XFePynWJjBjR8L9hZjMAiu87BluwvlxXm8p1iYwY0fA/CCwtbi8FHihnOCJSleG81XcX8DhwjpltNbNPAX8LXGFmG4Arip9F5CQy5OW97n7dIHctKHksIlIhXeEnkimFXyRT1c7qg9DspmaXLarX05H+fNg1LvYcOnpf7B9rfzu99JYd6gr11Xc41s7a0jet3tHB2ZGevv7brSfUl7eGmo1I2vOLZErhF8mUwi+SKYVfJFMKv0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKYVfJFOVT+wJlbay9NlAXeNjz2t756aP79Cs2OSXzudGhdp1bE1v43v3hfrq2xdr13Lhuclt9l4QW4+nt7+V3OZn+94T6itSOg4I7WZDk4gSNl/t+UUypfCLZErhF8lUtFbf18zseTNba2b3m9nE5g5TRMoWrdW3Ajjf3X8FeBH4YsnjEpEmC9Xqc/dH3P3I5yA9AcxswthEpInKeM1/PfDwYHeqXJfIyNRQ+M3sZqAHWD7YMirXJTIyhS/yMbOlwNXAAncPfCaviJxIofCb2ULgC8BvuPuBcockIlWI1ur7J6ATWGFmq83sO00ep4iULFqr7/YmjEVEKqQr/EQyVemsPjfoa09v1zMmvc3hSbHST52/tjO5zZKZ60J9LR83P9Rux6HO5DZTRs0K9dV6cHqo3aZrJyS3+Z2LHw/1FSm99bMdc0J9tR2IbVe9ge2+LzDp0zWrT0SGovCLZErhF8mUwi+SKYVfJFMKv0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKYVfJFPV1uprgd7ADL2kAmSFno5IP9A5+nBym/PHBornAZ+9IPYhSGtmp8/Qe/G3p4b66kuZJlbnY9OfSW5z6fiNob5+vv+s5Davbzk11Fdn+uYBxLb73kgpR83qE5GhKPwimQqV66q773Nm5mY2pTnDE5FmiZbrwsxmAVcAr5Q8JhGpQKhcV+EbwOcBfWa/yEko9JrfzK4BXnP3NcNY9hflug6oXJfISJH8Vp+ZdQA3A1cOZ3l3vxW4FWDsjFk6ShAZISJ7/rOAOcAaM9tMrULvKjOLfcyriJwQyXt+d38GOO3Iz8UTwHx3f7PEcYlIk0XLdYnISS5arqv+/tmljUZEKqMr/EQydVKU64po6Y612/TKaUMv1M+9Yy8O9XXhhNiEoKsnD/kO6zEmTo29zdrrsf3DAR+d3OaJd94T6uuBTRcktxm9ozXUV3CeU6z0ViCdKtclIkNS+EUypfCLZErhF8mUwi+SKYVfJFMKv0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKXOv7mP1zGwnsGWQu6cAI+HTgDSOo2kcRxvp4zjT3YdVm63S8B+PmT3l7vM1Do1D46hmHDrsF8mUwi+SqZEU/ltP9AAKGsfRNI6j/b8Zx4h5zS8i1RpJe34RqVCl4TezhWb2gpltNLNlA9w/2sx+UNy/0sxmN2EMs8zsJ2a23szWmdmNAyzzQTPbY2ari6+/LnscdX1tNrNnin6eGuB+M7N/KNbJWjObV3L/59T9n6vNbK+Z3dRvmaatj4FKwJvZZDNbYWYbiu+TBmm7tFhmg5ktbcI4vmZmzxfr/X4zmzhI2+M+hiWM48tm9lrd+l80SNvj5usY7l7JF9AKvATMBUYBa4Dz+i3zWeA7xe0lwA+aMI4ZwLzidifw4gDj+CDww4rWy2ZgynHuXwQ8DBhwKbCyyY/R69TeK65kfQCXA/OAZ+t+91VgWXF7GfCVAdpNBjYV3ycVtyeVPI4rgbbi9lcGGsdwHsMSxvFl4HPDeOyOm6/+X1Xu+S8BNrr7JnfvAu4GFvdbZjFwZ3H7HmCBmQU/LHlg7r7d3VcVt/cB64EzyuyjZIuB73vNE8BEM5vRpL4WAC+5+2AXYpXOBy4BX78d3Al8dICmHwFWuPtud38LWAEsLHMc7v6Iu/cUPz5BrS5lUw2yPoZjOPk6SpXhPwN4te7nrRwbuneXKVb6HuDUZg2oeFlxEbBygLvfb2ZrzOxhM3tfs8YAOPCImT1tZp8e4P7hrLeyLAHuGuS+qtYHwDR33w61J2vqakPWqXK9AFxP7QhsIEM9hmW4oXj5cccgL4OS10eV4R9oD97/rYbhLFMKMxsP3Avc5O57+929itqh74XAPwL/3owxFD7g7vOAq4A/MbPL+w91gDalrxMzGwVcA/zbAHdXuT6Gq8pt5WagB1g+yCJDPYaN+ja16ti/CmwH/n6gYQ7wu+OujyrDvxWYVffzTGDbYMuYWRtwCrFDoOMys3ZqwV/u7vf1v9/d97r7O8Xth4B2M5tS9jiKv7+t+L4DuJ/a4Vu94ay3MlwFrHL3NwYYY2Xro/DGkZc2xfcdAyxTyXopTiReDfyeFy+u+xvGY9gQd3/D3XvdvQ/450H+fvL6qDL8TwJnm9mcYi+zBHiw3zIPAkfO2n4c+PFgKzyqOIdwO7De3b8+yDLTj5xrMLNLqK2nXWWOo/jb48ys88htaieYnu232IPAHxRn/S8F9hw5JC7ZdQxyyF/V+qhTvx0sBR4YYJkfAVea2aTiMPjK4nelMbOFwBeAa9z9wCDLDOcxbHQc9ed4rh3k7w8nX0cr4wxlwpnMRdTOrr8E3Fz87m+orVyAMdQOOzcCPwfmNmEMv07tcGgtsLr4WgR8BvhMscwNwDpqZ0yfAC5r0vqYW/SxpujvyDqpH4sBtxTr7BlgfhPG0UEtzKfU/a6S9UHtCWc70E1t7/Upaud5HgU2FN8nF8vOB26ra3t9sa1sBD7ZhHFspPY6+sh2cuSdqNOBh473GJY8jn8pHvu11AI9o/84BsvX8b50hZ9IpnSFn0imFH6RTCn8IplS+EUypfCLZErhF8mUwi+SKYVfJFP/ByhvId3d2uzSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "i = 45 # index of data object....\n",
    "obj = Xtrain[i][:-1]\n",
    "plt.imshow(obj.reshape(16,16))\n",
    "plt.show()\n",
    "print(Ytrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bonsai():\n",
    "    def __init__(self, nClasses, dDims, pDims, tDepth, sigma, W=None, T=None, V=None, Z=None):\n",
    "        '''\n",
    "        dDims : data Dimensions\n",
    "        pDims : projected Dimesions\n",
    "        nClasses : num Classes\n",
    "        tDepth : tree Depth\n",
    "        \n",
    "        Expected Dimensions:\n",
    "        --------------------\n",
    "        Bonsai Params // Optional\n",
    "        \n",
    "        W [numClasses*totalNodes, projectionDimension]\n",
    "        V [numClasses*totalNodes, projectionDimension]\n",
    "        Z [projectionDimension, dataDimension + 1]\n",
    "        T [internalNodes, projectionDimension]\n",
    "\n",
    "        internalNodes = 2**treeDepth - 1\n",
    "        totalNodes = 2*internalNodes + 1\n",
    "\n",
    "        sigma - tanh non-linearity\n",
    "        sigmaI - Indicator function for node probabilities\n",
    "        sigmaI - has to be set to infinity(1e9 for practicality)\n",
    "        \n",
    "        while doing testing/inference\n",
    "        numClasses will be reset to 1 in binary case\n",
    "        '''\n",
    "        \n",
    "        # Initialization of parameter variables\n",
    "        \n",
    "        self.dDims = dDims\n",
    "        self.pDims = pDims\n",
    "\n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if nClasses == 2:\n",
    "            self.nClasses = 1\n",
    "        else:\n",
    "            self.nClasses = nClasses\n",
    "\n",
    "        self.tDepth = tDepth\n",
    "        self.sigma = sigma\n",
    "        self.iNodes = 2**self.tDepth - 1\n",
    "        self.tNodes = 2*self.iNodes + 1\n",
    "        \n",
    "        self.Z = tf.Variable(tf.random_normal([self.pDims, self.dDims]), name='Z', dtype=tf.float32)\n",
    "        self.W = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='W', dtype=tf.float32)\n",
    "        self.V = tf.Variable(tf.random_normal([self.nClasses * self.tNodes, self.pDims]), name='V', dtype=tf.float32)\n",
    "        self.T = tf.Variable(tf.random_normal([self.iNodes, self.pDims]), name='T', dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        self.assert_params()\n",
    "        \n",
    "        self.score = None\n",
    "        self.X_ = None\n",
    "        self.prediction = None\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, sigmaI):\n",
    "        '''\n",
    "        Function to build the Bonsai Tree graph\n",
    "        \n",
    "        Expected Dimensions\n",
    "        -------------------\n",
    "        X is [_, self.dDims]\n",
    "        X_ is [_, self.pDims]\n",
    "        '''\n",
    "        errmsg = \"Dimension Mismatch, X is [_, self.dataDimension]\"\n",
    "        assert (len(X.shape) == 2 and int(X.shape[1]) == self.dDims), errmsg\n",
    "        \n",
    "        \n",
    "        # return score, X_ if exists where X_ is the projected X, i.e X_ = (Z.X)/(D^)\n",
    "        if self.score is not None:\n",
    "            return self.score, self.X_\n",
    "        \n",
    "        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),self.pDims) # dimensions are D^x1\n",
    "        \n",
    "        \n",
    "        # For Root Node score...\n",
    "        self.__nodeProb = [] # node probability list\n",
    "        self.__nodeProb.append(1) # probability of x passing through root is 1.\n",
    "        W_ = self.W[0:(self.nClasses)]# first K trees root W params : KxD^\n",
    "        V_ = self.V[0:(self.nClasses)]# first K trees root V params : KxD^\n",
    "        \n",
    "        # All score sums variable initialized to root score... for each tree (Note: can be negative)\n",
    "        score_ = self.__nodeProb[0]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # : Kx1\n",
    "        \n",
    "        # Adding rest of the nodes scores...\n",
    "        for i in range(1, self.tNodes):\n",
    "            # current node is i\n",
    "            # W, V of K different trees for current node\n",
    "            W_ = self.W[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            V_ = self.V[i * self.nClasses:((i + 1) * self.nClasses)]# : KxD^\n",
    "            \n",
    "            # i's parent node shared theta param reshaping to 1xD^\n",
    "            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],[-1, self.pDims])# : 1xD^\n",
    "            \n",
    "            # Calculating probability that x should come to this node next given it is in parent node...\n",
    "            prob = tf.divide((1 + ((-1)**(i + 1))*tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_)))),2.0) # : scalar 1x1\n",
    "            # Actual probability that x will come to this node...p(parent)*p(this|parent)...\n",
    "            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob # : scalar 1x1\n",
    "            \n",
    "            # adding prob to node prob list\n",
    "            self.__nodeProb.append(prob)\n",
    "            # New score addes to sum of scores...\n",
    "            score_ += self.__nodeProb[i]*tf.multiply(tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_))) # Kx1\n",
    "            \n",
    "            \n",
    "        self.score = score_\n",
    "        self.X_ = X_\n",
    "        return self.score, self.X_\n",
    "    \n",
    "    \n",
    "    def predict(self):\n",
    "        '''\n",
    "        Takes in a score tensor and outputs a integer class for each data point\n",
    "        '''\n",
    "        if self.prediction is not None:\n",
    "            return self.prediction\n",
    "        \n",
    "        # If number of classes is two we dont need to calculate other class probability\n",
    "        if self.nClasses > 2:\n",
    "            # Finding argmax over first axis (k axis)\n",
    "            self.prediction = tf.argmax(tf.transpose(self.score), 1) # score is kx1\n",
    "        else:\n",
    "            # Finding argmax over score and 0 score is 1x1\n",
    "            self.prediction = tf.argmax(tf.concat([tf.transpose(self.score),0*tf.transpose(self.score)], 1), 1)\n",
    "        return self.prediction\n",
    "\n",
    "    def assert_params(self):\n",
    "        \n",
    "        # Asserting Initializaiton\n",
    "        \n",
    "        errRank = \"All Parameters must has only two dimensions shape = [a, b]\"\n",
    "        assert len(self.W.shape) == len(self.Z.shape), errRank\n",
    "        assert len(self.W.shape) == len(self.T.shape), errRank\n",
    "        assert len(self.W.shape) == 2, errRank\n",
    "        msg = \"W and V should be of same Dimensions\"\n",
    "        assert self.W.shape == self.V.shape, msg\n",
    "        errW = \"W and V are [numClasses*totalNodes, projectionDimension]\"\n",
    "        assert self.W.shape[0] == self.nClasses * self.tNodes, errW\n",
    "        assert self.W.shape[1] == self.pDims, errW\n",
    "        errZ = \"Z is [projectionDimension, dataDimension]\"\n",
    "        assert self.Z.shape[0] == self.pDims, errZ\n",
    "        assert self.Z.shape[1] == self.dDims, errZ\n",
    "        errT = \"T is [internalNodes, projectionDimension]\"\n",
    "        assert self.T.shape[0] == self.iNodes, errT\n",
    "        assert self.T.shape[1] == self.pDims, errT\n",
    "        assert int(self.nClasses) > 0, \"numClasses should be > 1\"\n",
    "        msg = \"# of features in data should be > 0\"\n",
    "        assert int(self.dDims) > 0, msg\n",
    "        msg = \"Projection should be  > 0 dims\"\n",
    "        assert int(self.pDims) > 0, msg\n",
    "        msg = \"treeDepth should be >= 0\"\n",
    "        assert int(self.tDepth) >= 0, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BonsaiTrainer():\n",
    "    \n",
    "    def __init__(self, tree, lW, lT, lV, lZ, lr, X, Y, sW, sV, sZ, sT):\n",
    "        \n",
    "        '''\n",
    "        bonsaiObj - Initialised Bonsai Object and Graph...\n",
    "        lW, lT, lV and lZ are regularisers to Bonsai Params...\n",
    "        sW, sT, sV and sZ are sparsity factors to Bonsai Params...\n",
    "        lr - learningRate fro optimizer...\n",
    "        X is the Data Placeholder - Dims [_, dataDimension]\n",
    "        Y - Label placeholder for loss computation\n",
    "        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n",
    "        useMCHLoss - True - MultiClass - multiClassHingeLoss\n",
    "        useMCHLoss - False - MultiClass - crossEntropyLoss\n",
    "        '''\n",
    "        #  Intializations of training parameters\n",
    "        self.tree = tree\n",
    "        \n",
    "        # regularization params lambdas(l) (all are scalars)\n",
    "        self.lW = lW\n",
    "        self.lV = lV\n",
    "        self.lT = lT\n",
    "        self.lZ = lZ\n",
    "\n",
    "        # sparsity parameters (scalars all...) will be used to calculate percentiles to make other cells zero\n",
    "        self.sW = sW \n",
    "        self.sV = sV\n",
    "        self.sT = sT\n",
    "        self.sZ = sZ\n",
    "\n",
    "        # placeholders for inputs and labels\n",
    "        self.Y = Y # _ x nClasses\n",
    "        self.X = X # _ x D\n",
    "        \n",
    "        # learning rate\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Asserting initialization\n",
    "        self.assert_params()\n",
    "        \n",
    "        # place holder for path selection parameter sigmaI\n",
    "        self.sigmaI = tf.placeholder(tf.float32, name='sigmaI')\n",
    "        # invoking __call__ of tree getting initial values of score and projected X\n",
    "        self.score, self.X_ = self.tree(self.X, self.sigmaI)\n",
    "        # defining loss function tensorflow graph variables.....\n",
    "        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n",
    "        # defining single training step graph process ...\n",
    "        self.tree.TrainStep = tf.train.AdamOptimizer(self.lr).minimize(self.loss)\n",
    "        self.trainStep = self.tree.TrainStep\n",
    "        # defining accuracy and prediction graph objects\n",
    "        self.accuracy = self.accuracyGraph()\n",
    "        self.prediction = self.tree.predict()\n",
    "        \n",
    "        \n",
    "        # set all parameters above 0.99 if dont want to use IHT\n",
    "        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n",
    "            self.isDenseTraining = True\n",
    "        else:\n",
    "            self.isDenseTraining = False\n",
    "            \n",
    "        # setting the hard thresholding graph obejcts\n",
    "        self.hardThrsd()\n",
    "        \n",
    "    def hardThrsd(self):\n",
    "        '''\n",
    "        Set up for hard Thresholding Functionality\n",
    "        '''\n",
    "        # place holders for sparse parameters....\n",
    "        self.__Wth = tf.placeholder(tf.float32, name='Wth')\n",
    "        self.__Vth = tf.placeholder(tf.float32, name='Vth')\n",
    "        self.__Zth = tf.placeholder(tf.float32, name='Zth')\n",
    "        self.__Tth = tf.placeholder(tf.float32, name='Tth')\n",
    "\n",
    "        # assigning the thresholded values to params as a graph object for tensorflow....\n",
    "        self.__Woph = self.tree.W.assign(self.__Wth)\n",
    "        self.__Voph = self.tree.V.assign(self.__Vth)\n",
    "        self.__Toph = self.tree.T.assign(self.__Tth)\n",
    "        self.__Zoph = self.tree.Z.assign(self.__Zth)\n",
    "\n",
    "        # grouping the graph objects as one object....\n",
    "        self.hardThresholdGroup = tf.group(\n",
    "            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n",
    "        \n",
    "    def hardThreshold(self, A, s):\n",
    "        '''\n",
    "        Hard thresholding function on Tensor A with sparsity s\n",
    "        '''\n",
    "        # copying to avoid errors....\n",
    "        A_ = np.copy(A)\n",
    "        # flattening the tensor...\n",
    "        A_ = A_.ravel()\n",
    "        if len(A_) > 0:\n",
    "            # calculating the threshold value for sparse limit...\n",
    "            th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "            # making sparse.......\n",
    "            A_[np.abs(A_) < th] = 0.0\n",
    "        # reconstructing in actual shape....\n",
    "        A_ = A_.reshape(A.shape)\n",
    "        return A_\n",
    "\n",
    "    def accuracyGraph(self):\n",
    "        '''\n",
    "        Accuracy Graph to evaluate accuracy when needed\n",
    "        '''\n",
    "        if (self.tree.nClasses > 2):\n",
    "            correctPrediction = tf.equal(tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correctPrediction, tf.float32))\n",
    "        else:\n",
    "            # some accuracy functional analysis for 2 classes could be different from this...\n",
    "            y_ = self.Y * 2 - 1\n",
    "            correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n",
    "            correctPrediction = tf.nn.relu(correctPrediction)\n",
    "            correctPrediction = tf.ceil(tf.tanh(correctPrediction)) # final predictions.... round to(0 or 1)\n",
    "            self.accuracy = tf.reduce_mean(\n",
    "                tf.cast(correctPrediction, tf.float32))\n",
    "\n",
    "        return self.accuracy\n",
    "        \n",
    "    \n",
    "    def lossGraph(self):\n",
    "        '''\n",
    "        Loss Graph for given tree\n",
    "        '''\n",
    "        # regularization losses.....\n",
    "        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.tree.Z)) +\n",
    "                          self.lW * tf.square(tf.norm(self.tree.W)) +\n",
    "                          self.lV * tf.square(tf.norm(self.tree.V)) +\n",
    "                          self.lT * tf.square(tf.norm(self.tree.T)))\n",
    "        \n",
    "        # emperical actual loss.....\n",
    "        if (self.tree.nClasses > 2):\n",
    "            '''\n",
    "            Cross Entropy loss for MultiClass case in joint training for\n",
    "            faster convergence\n",
    "            '''\n",
    "            # cross entropy loss....\n",
    "            self.marginLoss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits_v2(logits=tf.transpose(self.score),\n",
    "                                                               labels=tf.stop_gradient(self.Y)))\n",
    "        else:\n",
    "            # sigmoid loss....\n",
    "            self.marginLoss = tf.reduce_mean(tf.nn.relu(1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n",
    "    \n",
    "        # adding the losses...\n",
    "        self.loss = self.marginLoss + self.regLoss\n",
    "        return self.loss, self.marginLoss, self.regLoss\n",
    "        \n",
    "    def assert_params(self):\n",
    "        # asserting the initialization....\n",
    "        err = \"sparsity must be between 0 and 1\"\n",
    "        assert self.sW >= 0 and self.sW <= 1, \"W \" + err\n",
    "        assert self.sV >= 0 and self.sV <= 1, \"V \" + err\n",
    "        assert self.sZ >= 0 and self.sZ <= 1, \"Z \" + err\n",
    "        assert self.sT >= 0 and self.sT <= 1, \"T \" + err\n",
    "        errMsg = \"Dimension Mismatch, Y has to be [_, \" + str(self.tree.nClasses) + \"]\"\n",
    "        errCont = \" numClasses are 1 in case of Binary case by design\"\n",
    "        assert (len(self.Y.shape) == 2 and self.Y.shape[1] == self.tree.nClasses), errMsg + errCont\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    def train(self, batchSize, totalEpochs, sess, Xtrain, Xval, Ytrain, Yval):\n",
    "        iht = 0 # to keep a note if thresholding has been started ...\n",
    "        numIters = Xtrain.shape[0] / batchSize # number of batches at a time...\n",
    "        totalBatches = numIters * totalEpochs # total number of batch operations...\n",
    "        treeSigmaI = 1 # controls the fidelity of the approximation too high can saturate tanh.\n",
    "            \n",
    "        maxTestAcc = -10000\n",
    "        itersInPhase = 0\n",
    "        \n",
    "        for i in range(totalEpochs):\n",
    "            print(\"\\nEpoch Number: \" + str(i))\n",
    "            # defining training acc and loss\n",
    "            trainAcc = 0.0\n",
    "            trainLoss = 0.0\n",
    "            \n",
    "            numIters = int(numIters)\n",
    "            \n",
    "            for j in range(numIters):\n",
    "                # creating batch.....sequentiall could be done randomly using choice function...\n",
    "                mini_batchX = Xtrain[j*batchSize:(j+1)*batchSize,:] # B x D\n",
    "                mini_batchY = Ytrain[j*batchSize:(j+1)*batchSize] # B x \n",
    "            \n",
    "                # feed for training using tensorflow graph based gradient descent approach......\n",
    "                _feed_dict = {self.X: mini_batchX, self.Y: mini_batchY,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "                \n",
    "                # training the tensorflow graph\n",
    "                _, batchLoss, batchAcc = sess.run(\n",
    "                    [self.trainStep, self.loss, self.accuracy],\n",
    "                    feed_dict=_feed_dict)\n",
    "                \n",
    "                # calculating acc....\n",
    "                trainAcc += batchAcc\n",
    "                trainLoss += batchLoss\n",
    "                \n",
    "                # to update sigmaI.....\n",
    "                if (itersInPhase % 100 == 0):\n",
    "                    \n",
    "                    # Making a random batch....\n",
    "                    indices = np.random.choice(Xtrain.shape[0], 100)\n",
    "                    rand_batchX = Xtrain[indices, :]\n",
    "                    rand_batchY = Ytrain[indices, :]\n",
    "                    rand_batchY = np.reshape(rand_batchY, [-1, self.tree.nClasses])\n",
    "\n",
    "                    _feed_dict = {self.X: rand_batchX}\n",
    "                    # Projected matrix...\n",
    "                    Xcapeval = self.X_.eval(feed_dict=_feed_dict) # D^ x 1\n",
    "                    # theta value... current...\n",
    "                    Teval = self.tree.T.eval() # iNodes x D^\n",
    "                    # current sum of all internal nodes sum(abs(theta^T.Z.x): iNoddes x miniBS) : 1x1\n",
    "                    sum_tr = 0.0 \n",
    "                    for k in range(0, self.tree.iNodes):\n",
    "                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n",
    "\n",
    "                    \n",
    "                    if(self.tree.iNodes > 0):\n",
    "                        sum_tr /= (100 * self.tree.iNodes) # normalizing all sums\n",
    "                        sum_tr = 0.1 / sum_tr # inverse of average sum\n",
    "                    else:\n",
    "                        sum_tr = 0.1\n",
    "                    # thresholding inverse of sum as min(1000, sum_inv*2^(cuurent batch number / total bacthes / 30))\n",
    "                    sum_tr = min(\n",
    "                        1000, sum_tr * (2**(float(itersInPhase) /\n",
    "                                            (float(totalBatches) / 30.0))))\n",
    "                    # assiging higher values as convergence is reached...\n",
    "                    treeSigmaI = sum_tr\n",
    "                    \n",
    "                itersInPhase+=1\n",
    "                \n",
    "                \n",
    "                # to start hard thresholding after half_time(could vary) ......\n",
    "                if((itersInPhase//numIters > (1/2)*totalEpochs) and (not self.isDenseTraining)):\n",
    "                    if(iht == 0):\n",
    "                        print('\\n\\nHard Thresolding Started\\n\\n')\n",
    "                        iht = 1\n",
    "                    \n",
    "                    # getting the current estimates of  W,V,Z,T...\n",
    "                    currW = self.tree.W.eval()\n",
    "                    currV = self.tree.V.eval()\n",
    "                    currZ = self.tree.Z.eval()\n",
    "                    currT = self.tree.T.eval()\n",
    "\n",
    "                    # Setting a method to make some values of matrix zero....\n",
    "                    self.__thrsdW = self.hardThreshold(currW, self.sW)\n",
    "                    self.__thrsdV = self.hardThreshold(currV, self.sV)\n",
    "                    self.__thrsdZ = self.hardThreshold(currZ, self.sZ)\n",
    "                    self.__thrsdT = self.hardThreshold(currT, self.sT)\n",
    "\n",
    "                    # runnign the hard thresholding graph....\n",
    "                    fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n",
    "                                self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n",
    "                    sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n",
    "                    \n",
    "            \n",
    "            \n",
    "            print(\"Train Loss: \" + str(trainLoss / numIters) +\n",
    "                  \" Train accuracy: \" + str(trainAcc / numIters))\n",
    "            \n",
    "            # calculating the test accuracies with sigmaI as expected -> inf.. = 10^9\n",
    "            oldSigmaI = treeSigmaI\n",
    "            treeSigmaI = 1e9\n",
    "            \n",
    "            # test feed for tf...\n",
    "            _feed_dict = {self.X: Xval, self.Y: Yval,\n",
    "                                  self.sigmaI: treeSigmaI}\n",
    "            \n",
    "            # calculating losses....\n",
    "            testAcc, testLoss, regTestLoss = sess.run([self.accuracy, self.loss, self.regLoss], feed_dict=_feed_dict)\n",
    "            \n",
    "            if maxTestAcc <= testAcc:\n",
    "                maxTestAccEpoch = i\n",
    "                maxTestAcc = testAcc\n",
    "            \n",
    "            print(\"Test accuracy %g\" % testAcc)\n",
    "            print(\"MarginLoss + RegLoss: \" + str(testLoss - regTestLoss) +\n",
    "                  \" + \" + str(regTestLoss) + \" = \" + str(testLoss) + \"\\n\", end='\\r')\n",
    "#             time.sleep(0.1)\n",
    "#             clear_output()\n",
    "            \n",
    "            \n",
    "            treeSigmaI = oldSigmaI\n",
    "            \n",
    "        # sigmaI has to be set to infinity to ensure\n",
    "        # only a single path is used in inference\n",
    "        treeSigmaI = 1e9\n",
    "        print(\"\\nMaximum Test accuracy at compressed\" +\n",
    "              \" model size(including early stopping): \" +\n",
    "              str(maxTestAcc) + \" at Epoch: \" +\n",
    "              str(maxTestAccEpoch + 1) + \"\\nFinal Test\" +\n",
    "              \" Accuracy: \" + str(testAcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = Bonsai(nClasses = nClasses, dDims = dDims, pDims = 28, tDepth = 3, sigma = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float32\", [None, dDims])\n",
    "Y = tf.placeholder(\"float32\", [None, nClasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonsaiTrainer = BonsaiTrainer(tree, lW = 0.001, lT = 0.001, lV = 0.001, lZ = 0.0001, lr = 0.01, X = X, Y = Y,\n",
    "                              sZ = 0.2, sW = 0.3, sV = 0.3, sT = 0.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch Number: 0\n",
      "Train Loss: 5.302061849170261 Train accuracy: 0.6787500028601952\n",
      "Test accuracy 0.679123\n",
      "MarginLoss + RegLoss: 1.4188669 + 3.2358024 = 4.6546693\n",
      "\n",
      "Epoch Number: 1\n",
      "Train Loss: 2.985516303115421 Train accuracy: 0.9087499984436564\n",
      "Test accuracy 0.809168\n",
      "MarginLoss + RegLoss: 0.83106375 + 2.1322556 = 2.9633193\n",
      "\n",
      "Epoch Number: 2\n",
      "Train Loss: 1.9790794865952597 Train accuracy: 0.9443055589993795\n",
      "Test accuracy 0.856004\n",
      "MarginLoss + RegLoss: 0.61750066 + 1.441271 = 2.0587716\n",
      "\n",
      "Epoch Number: 3\n",
      "Train Loss: 1.3906004163953993 Train accuracy: 0.9581944503717952\n",
      "Test accuracy 0.879422\n",
      "MarginLoss + RegLoss: 0.48983884 + 1.0285778 = 1.5184166\n",
      "\n",
      "Epoch Number: 4\n",
      "Train Loss: 1.0417863263024225 Train accuracy: 0.965138895644082\n",
      "Test accuracy 0.900847\n",
      "MarginLoss + RegLoss: 0.42124474 + 0.78116775 = 1.2024125\n",
      "\n",
      "Epoch Number: 5\n",
      "Train Loss: 0.8287534200482898 Train accuracy: 0.9700000079141723\n",
      "Test accuracy 0.906328\n",
      "MarginLoss + RegLoss: 0.39114374 + 0.6287977 = 1.0199414\n",
      "\n",
      "Epoch Number: 6\n",
      "Train Loss: 0.6929824343985982 Train accuracy: 0.9715277825792631\n",
      "Test accuracy 0.912307\n",
      "MarginLoss + RegLoss: 0.3670156 + 0.5307239 = 0.8977395\n",
      "\n",
      "Epoch Number: 7\n",
      "Train Loss: 0.599840966363748 Train accuracy: 0.9733333397242758\n",
      "Test accuracy 0.915795\n",
      "MarginLoss + RegLoss: 0.34784967 + 0.46248597 = 0.81033564\n",
      "\n",
      "Epoch Number: 8\n",
      "Train Loss: 0.5331467758450243 Train accuracy: 0.9755555623107486\n",
      "Test accuracy 0.917289\n",
      "MarginLoss + RegLoss: 0.3365826 + 0.4126354 = 0.749218\n",
      "\n",
      "Epoch Number: 9\n",
      "Train Loss: 0.485456215010749 Train accuracy: 0.9769444531864591\n",
      "Test accuracy 0.916293\n",
      "MarginLoss + RegLoss: 0.3277501 + 0.3760785 = 0.7038286\n",
      "\n",
      "Epoch Number: 10\n",
      "Train Loss: 0.44574659027987057 Train accuracy: 0.9787500086757872\n",
      "Test accuracy 0.919282\n",
      "MarginLoss + RegLoss: 0.32079363 + 0.3466468 = 0.6674404\n",
      "\n",
      "Epoch Number: 11\n",
      "Train Loss: 0.4142606353594197 Train accuracy: 0.9808333449893527\n",
      "Test accuracy 0.922272\n",
      "MarginLoss + RegLoss: 0.3147603 + 0.3219991 = 0.6367594\n",
      "\n",
      "Epoch Number: 12\n",
      "Train Loss: 0.3864413880639606 Train accuracy: 0.982638897995154\n",
      "Test accuracy 0.924763\n",
      "MarginLoss + RegLoss: 0.30985397 + 0.30014092 = 0.6099949\n",
      "\n",
      "Epoch Number: 13\n",
      "Train Loss: 0.363297617683808 Train accuracy: 0.983055565920141\n",
      "Test accuracy 0.92576\n",
      "MarginLoss + RegLoss: 0.30613896 + 0.28162268 = 0.58776164\n",
      "\n",
      "Epoch Number: 14\n",
      "Train Loss: 0.3434128525356452 Train accuracy: 0.9841666776272986\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: 0.29995954 + 0.26490223 = 0.5648618\n",
      "\n",
      "Epoch Number: 15\n",
      "Train Loss: 0.3252903243733777 Train accuracy: 0.9833333459165361\n",
      "Test accuracy 0.926756\n",
      "MarginLoss + RegLoss: 0.29361004 + 0.24952477 = 0.5431348\n",
      "\n",
      "Epoch Number: 16\n",
      "Train Loss: 0.3102971381611294 Train accuracy: 0.9850000109937456\n",
      "Test accuracy 0.927255\n",
      "MarginLoss + RegLoss: 0.29406905 + 0.23671497 = 0.530784\n",
      "\n",
      "Epoch Number: 17\n",
      "Train Loss: 0.29467842396762634 Train accuracy: 0.9861111218730608\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: 0.2968725 + 0.22497632 = 0.5218488\n",
      "\n",
      "Epoch Number: 18\n",
      "Train Loss: 0.28320072818961406 Train accuracy: 0.985138900578022\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.28451505 + 0.21471012 = 0.49922517\n",
      "\n",
      "Epoch Number: 19\n",
      "Train Loss: 0.27878696471452713 Train accuracy: 0.9823611221379704\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: 0.29433417 + 0.20745434 = 0.5017885\n",
      "\n",
      "Epoch Number: 20\n",
      "Train Loss: 0.27802268891698784 Train accuracy: 0.9819444533851411\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: 0.2786424 + 0.20144248 = 0.48008487\n",
      "\n",
      "Epoch Number: 21\n",
      "Train Loss: 0.2625275310128927 Train accuracy: 0.9834722313616011\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: 0.26777148 + 0.19264841 = 0.4604199\n",
      "\n",
      "Epoch Number: 22\n",
      "Train Loss: 0.2567250397470262 Train accuracy: 0.983055565920141\n",
      "Test accuracy 0.927255\n",
      "MarginLoss + RegLoss: 0.28678033 + 0.18626592 = 0.47304624\n",
      "\n",
      "Epoch Number: 23\n",
      "Train Loss: 0.2560780009047853 Train accuracy: 0.9822222317258517\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: 0.26807398 + 0.18235852 = 0.45043248\n",
      "\n",
      "Epoch Number: 24\n",
      "Train Loss: 0.24837696096963352 Train accuracy: 0.982638897995154\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.2720789 + 0.17619982 = 0.44827873\n",
      "\n",
      "Epoch Number: 25\n",
      "Train Loss: 0.24144918988976213 Train accuracy: 0.9830555684036679\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: 0.27596885 + 0.173317 = 0.44928586\n",
      "\n",
      "Epoch Number: 26\n",
      "Train Loss: 0.24317450531654888 Train accuracy: 0.9822222333815362\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: 0.27436668 + 0.16947307 = 0.44383973\n",
      "\n",
      "Epoch Number: 27\n",
      "Train Loss: 0.23825008504920536 Train accuracy: 0.982638900478681\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: 0.26574355 + 0.16504632 = 0.4307899\n",
      "\n",
      "Epoch Number: 28\n",
      "Train Loss: 0.2300886395904753 Train accuracy: 0.9837500105301539\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: 0.27750304 + 0.162669 = 0.44017205\n",
      "\n",
      "Epoch Number: 29\n",
      "Train Loss: 0.2349975932803419 Train accuracy: 0.9811111216743787\n",
      "Test accuracy 0.926756\n",
      "MarginLoss + RegLoss: 0.2840164 + 0.16147918 = 0.44549558\n",
      "\n",
      "Epoch Number: 30\n",
      "Train Loss: 0.22478095690409342 Train accuracy: 0.9831944555044174\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: 0.2868994 + 0.15707578 = 0.44397515\n",
      "\n",
      "Epoch Number: 31\n",
      "Train Loss: 0.22697015644775498 Train accuracy: 0.9831944521930482\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: 0.2647956 + 0.15541704 = 0.42021263\n",
      "\n",
      "Epoch Number: 32\n",
      "Train Loss: 0.22212047999103865 Train accuracy: 0.9833333442608515\n",
      "Test accuracy 0.935227\n",
      "MarginLoss + RegLoss: 0.27638867 + 0.15376064 = 0.43014932\n",
      "\n",
      "Epoch Number: 33\n",
      "Train Loss: 0.22206336735851234 Train accuracy: 0.9831944563322597\n",
      "Test accuracy 0.923767\n",
      "MarginLoss + RegLoss: 0.29105753 + 0.15190257 = 0.44296008\n",
      "\n",
      "Epoch Number: 34\n",
      "Train Loss: 0.24363598889774746 Train accuracy: 0.9772222340106964\n",
      "Test accuracy 0.924763\n",
      "MarginLoss + RegLoss: 0.28862298 + 0.1572001 = 0.44582307\n",
      "\n",
      "Epoch Number: 35\n",
      "Train Loss: 0.22786218900647429 Train accuracy: 0.9806944578886032\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.26350772 + 0.15584296 = 0.41935068\n",
      "\n",
      "Epoch Number: 36\n",
      "Train Loss: 0.22014755196869373 Train accuracy: 0.981527790427208\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: 0.27460715 + 0.15103534 = 0.4256425\n",
      "\n",
      "Epoch Number: 37\n",
      "Train Loss: 0.22048028061787286 Train accuracy: 0.9805555649929576\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.2574957 + 0.14946051 = 0.4069562\n",
      "\n",
      "Epoch Number: 38\n",
      "Train Loss: 0.21275700483885077 Train accuracy: 0.9825000125500891\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.25769198 + 0.14726949 = 0.40496147\n",
      "\n",
      "Epoch Number: 39\n",
      "Train Loss: 0.2104672814408938 Train accuracy: 0.984166675971614\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.25710243 + 0.14548546 = 0.4025879\n",
      "\n",
      "Epoch Number: 40\n",
      "Train Loss: 0.22798319471379122 Train accuracy: 0.9777777848972214\n",
      "Test accuracy 0.925262\n",
      "MarginLoss + RegLoss: 0.28064644 + 0.15360995 = 0.43425637\n",
      "\n",
      "Epoch Number: 41\n",
      "Train Loss: 0.2434882428497076 Train accuracy: 0.9776389002799988\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: 0.26002455 + 0.1573314 = 0.41735595\n",
      "\n",
      "Epoch Number: 42\n",
      "Train Loss: 0.236241669083635 Train accuracy: 0.9798611212107871\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.2750242 + 0.16146928 = 0.4364935\n",
      "\n",
      "Epoch Number: 43\n",
      "Train Loss: 0.22374588458074463 Train accuracy: 0.9833333426051669\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: 0.27153105 + 0.15535672 = 0.42688775\n",
      "\n",
      "Epoch Number: 44\n",
      "Train Loss: 0.2147238782296578 Train accuracy: 0.9859722322887845\n",
      "Test accuracy 0.927753\n",
      "MarginLoss + RegLoss: 0.26623344 + 0.15233316 = 0.41856658\n",
      "\n",
      "Epoch Number: 45\n",
      "Train Loss: 0.22734644843472374 Train accuracy: 0.9829166738523377\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: 0.26368037 + 0.15803438 = 0.42171475\n",
      "\n",
      "Epoch Number: 46\n",
      "Train Loss: 0.22635010961029264 Train accuracy: 0.9834722330172857\n",
      "Test accuracy 0.934728\n",
      "MarginLoss + RegLoss: 0.2710575 + 0.15661475 = 0.42767224\n",
      "\n",
      "Epoch Number: 47\n",
      "Train Loss: 0.22301151375803682 Train accuracy: 0.9830555650922987\n",
      "Test accuracy 0.934728\n",
      "MarginLoss + RegLoss: 0.26916593 + 0.15505587 = 0.4242218\n",
      "\n",
      "Epoch Number: 48\n",
      "Train Loss: 0.26894866364697617 Train accuracy: 0.9715277842349477\n",
      "Test accuracy 0.915296\n",
      "MarginLoss + RegLoss: 0.34936702 + 0.17039436 = 0.5197614\n",
      "\n",
      "Epoch Number: 49\n",
      "Train Loss: 0.2792867376572556 Train accuracy: 0.9708333387970924\n",
      "Test accuracy 0.922272\n",
      "MarginLoss + RegLoss: 0.30595386 + 0.17261772 = 0.4785716\n",
      "\n",
      "Epoch Number: 50\n",
      "\n",
      "\n",
      "Hard Thresolding Started\n",
      "\n",
      "\n",
      "Train Loss: 0.36008837736315197 Train accuracy: 0.9540277802281909\n",
      "Test accuracy 0.880917\n",
      "MarginLoss + RegLoss: 0.47275895 + 0.17812866 = 0.6508876\n",
      "\n",
      "Epoch Number: 51\n",
      "Train Loss: 0.3614614142311944 Train accuracy: 0.9541666714681519\n",
      "Test accuracy 0.923767\n",
      "MarginLoss + RegLoss: 0.30630067 + 0.18527737 = 0.49157804\n",
      "\n",
      "Epoch Number: 52\n",
      "Train Loss: 0.3059246995382839 Train accuracy: 0.9681944515970018\n",
      "Test accuracy 0.927255\n",
      "MarginLoss + RegLoss: 0.29124284 + 0.178846 = 0.47008884\n",
      "\n",
      "Epoch Number: 53\n",
      "Train Loss: 0.28368763832582367 Train accuracy: 0.9741666747464074\n",
      "Test accuracy 0.926756\n",
      "MarginLoss + RegLoss: 0.2860651 + 0.17594926 = 0.46201438\n",
      "\n",
      "Epoch Number: 54\n",
      "Train Loss: 0.2765533957216475 Train accuracy: 0.9765277852614721\n",
      "Test accuracy 0.924265\n",
      "MarginLoss + RegLoss: 0.28583091 + 0.17266525 = 0.45849615\n",
      "\n",
      "Epoch Number: 55\n",
      "Train Loss: 0.26665473398235107 Train accuracy: 0.9761111181643274\n",
      "Test accuracy 0.926756\n",
      "MarginLoss + RegLoss: 0.2776553 + 0.17051385 = 0.44816917\n",
      "\n",
      "Epoch Number: 56\n",
      "Train Loss: 0.26084884794221985 Train accuracy: 0.9783333390951157\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.2717811 + 0.1693941 = 0.4411752\n",
      "\n",
      "Epoch Number: 57\n",
      "Train Loss: 0.25447098248534733 Train accuracy: 0.9793055628736814\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: 0.27126634 + 0.16772714 = 0.43899348\n",
      "\n",
      "Epoch Number: 58\n",
      "Train Loss: 0.2519329411702024 Train accuracy: 0.9805555641651154\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.28399703 + 0.16786444 = 0.45186147\n",
      "\n",
      "Epoch Number: 59\n",
      "Train Loss: 0.25430366458992165 Train accuracy: 0.9802777858244048\n",
      "Test accuracy 0.927753\n",
      "MarginLoss + RegLoss: 0.28480542 + 0.16777493 = 0.45258033\n",
      "\n",
      "Epoch Number: 60\n",
      "Train Loss: 0.2464908386270205 Train accuracy: 0.9813889000150893\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.28887117 + 0.16727586 = 0.45614702\n",
      "\n",
      "Epoch Number: 61\n",
      "Train Loss: 0.2575535358240207 Train accuracy: 0.9770833394593663\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: 0.27366734 + 0.16861202 = 0.44227934\n",
      "\n",
      "Epoch Number: 62\n",
      "Train Loss: 0.2498890697542164 Train accuracy: 0.9806944529215494\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: 0.26780054 + 0.16706339 = 0.43486392\n",
      "\n",
      "Epoch Number: 63\n",
      "Train Loss: 0.24535448207623428 Train accuracy: 0.9815277862879965\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: 0.26368147 + 0.16620573 = 0.4298872\n",
      "\n",
      "Epoch Number: 64\n",
      "Train Loss: 0.24217270211213165 Train accuracy: 0.9811111175351672\n",
      "Test accuracy 0.933732\n",
      "MarginLoss + RegLoss: 0.2608173 + 0.16594724 = 0.42676455\n",
      "\n",
      "Epoch Number: 65\n",
      "Train Loss: 0.24136144378119045 Train accuracy: 0.982083341313733\n",
      "Test accuracy 0.934728\n",
      "MarginLoss + RegLoss: 0.25730944 + 0.16539738 = 0.4227068\n",
      "\n",
      "Epoch Number: 66\n",
      "Train Loss: 0.23922052772508728 Train accuracy: 0.9823611196544435\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: 0.259119 + 0.16446072 = 0.42357972\n",
      "\n",
      "Epoch Number: 67\n",
      "Train Loss: 0.24542222378982437 Train accuracy: 0.9788888974322213\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.26428548 + 0.16594982 = 0.4302353\n",
      "\n",
      "Epoch Number: 68\n",
      "Train Loss: 0.2462113400300344 Train accuracy: 0.9805555625094308\n",
      "Test accuracy 0.93423\n",
      "MarginLoss + RegLoss: 0.26115856 + 0.16426262 = 0.42542118\n",
      "\n",
      "Epoch Number: 69\n",
      "Train Loss: 0.2412648523847262 Train accuracy: 0.9800000074836943\n",
      "Test accuracy 0.934728\n",
      "MarginLoss + RegLoss: 0.26135007 + 0.1631015 = 0.42445156\n",
      "\n",
      "Epoch Number: 70\n",
      "Train Loss: 0.2385153522094091 Train accuracy: 0.9800000050001674\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.26214284 + 0.16206022 = 0.42420304\n",
      "\n",
      "Epoch Number: 71\n",
      "Train Loss: 0.2424225662317541 Train accuracy: 0.9791666757729318\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.26683477 + 0.16517714 = 0.4320119\n",
      "\n",
      "Epoch Number: 72\n",
      "Train Loss: 0.2477055098861456 Train accuracy: 0.9786111174358262\n",
      "Test accuracy 0.932735\n",
      "MarginLoss + RegLoss: 0.25671515 + 0.16405976 = 0.4207749\n",
      "\n",
      "Epoch Number: 73\n",
      "Train Loss: 0.24061198511885273 Train accuracy: 0.9801388954122862\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.25909966 + 0.16309004 = 0.4221897\n",
      "\n",
      "Epoch Number: 74\n",
      "Train Loss: 0.23887825964225662 Train accuracy: 0.9800000083115366\n",
      "Test accuracy 0.933234\n",
      "MarginLoss + RegLoss: 0.26253882 + 0.16226393 = 0.42480275\n",
      "\n",
      "Epoch Number: 75\n",
      "Train Loss: 0.23571537517839009 Train accuracy: 0.980277783340878\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: 0.2701134 + 0.1613953 = 0.43150872\n",
      "\n",
      "Epoch Number: 76\n",
      "Train Loss: 0.23677353147003385 Train accuracy: 0.9812500054637591\n",
      "Test accuracy 0.92576\n",
      "MarginLoss + RegLoss: 0.27446797 + 0.16117433 = 0.4356423\n",
      "\n",
      "Epoch Number: 77\n",
      "Train Loss: 0.23412490470541847 Train accuracy: 0.9819444509016143\n",
      "Test accuracy 0.926258\n",
      "MarginLoss + RegLoss: 0.2711113 + 0.16078435 = 0.43189567\n",
      "\n",
      "Epoch Number: 78\n",
      "Train Loss: 0.23201305584775078 Train accuracy: 0.981666673388746\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.27030578 + 0.16006756 = 0.43037334\n",
      "\n",
      "Epoch Number: 79\n",
      "Train Loss: 0.23345290765994126 Train accuracy: 0.9812500071194437\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.26814973 + 0.15968 = 0.42782974\n",
      "\n",
      "Epoch Number: 80\n",
      "Train Loss: 0.232561477770408 Train accuracy: 0.9813888967037201\n",
      "Test accuracy 0.927753\n",
      "MarginLoss + RegLoss: 0.27186885 + 0.15902987 = 0.43089873\n",
      "\n",
      "Epoch Number: 81\n",
      "Train Loss: 0.23227527344392407 Train accuracy: 0.9809722296065755\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.2719471 + 0.15854 = 0.4304871\n",
      "\n",
      "Epoch Number: 82\n",
      "Train Loss: 0.22957015617026222 Train accuracy: 0.981666673388746\n",
      "Test accuracy 0.928251\n",
      "MarginLoss + RegLoss: 0.2713065 + 0.15766451 = 0.42897102\n",
      "\n",
      "Epoch Number: 83\n",
      "Train Loss: 0.22837554849684238 Train accuracy: 0.9818055621451802\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: 0.26691228 + 0.15709119 = 0.42400348\n",
      "\n",
      "Epoch Number: 84\n",
      "Train Loss: 0.2273505905436145 Train accuracy: 0.9829166746801801\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.26849574 + 0.15639298 = 0.42488873\n",
      "\n",
      "Epoch Number: 85\n",
      "Train Loss: 0.22615793657799563 Train accuracy: 0.9827777850959036\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.26643875 + 0.15578812 = 0.42222688\n",
      "\n",
      "Epoch Number: 86\n",
      "Train Loss: 0.22635753225121233 Train accuracy: 0.9823611196544435\n",
      "Test accuracy 0.928749\n",
      "MarginLoss + RegLoss: 0.2763596 + 0.15599358 = 0.43235317\n",
      "\n",
      "Epoch Number: 87\n",
      "Train Loss: 0.22721241600811481 Train accuracy: 0.9827777859237459\n",
      "Test accuracy 0.929746\n",
      "MarginLoss + RegLoss: 0.26991466 + 0.15539351 = 0.42530817\n",
      "\n",
      "Epoch Number: 88\n",
      "Train Loss: 0.22849489996830621 Train accuracy: 0.9815277846323119\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.2609306 + 0.15558867 = 0.41651928\n",
      "\n",
      "Epoch Number: 89\n",
      "Train Loss: 0.22600893614192805 Train accuracy: 0.9825000084108777\n",
      "Test accuracy 0.932237\n",
      "MarginLoss + RegLoss: 0.26451957 + 0.15515582 = 0.4196754\n",
      "\n",
      "Epoch Number: 90\n",
      "Train Loss: 0.22400229341453975 Train accuracy: 0.9834722305337588\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.26258594 + 0.15452127 = 0.41710722\n",
      "\n",
      "Epoch Number: 91\n",
      "Train Loss: 0.22327617410984305 Train accuracy: 0.983194451365206\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.26711825 + 0.15414381 = 0.42126206\n",
      "\n",
      "Epoch Number: 92\n",
      "Train Loss: 0.2227320886320538 Train accuracy: 0.9843055614166789\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: 0.27012384 + 0.15383764 = 0.42396146\n",
      "\n",
      "Epoch Number: 93\n",
      "Train Loss: 0.22179465202821624 Train accuracy: 0.9843055630723635\n",
      "Test accuracy 0.931241\n",
      "MarginLoss + RegLoss: 0.26597375 + 0.15359992 = 0.41957366\n",
      "\n",
      "Epoch Number: 94\n",
      "Train Loss: 0.22215205762121412 Train accuracy: 0.9838888951473765\n",
      "Test accuracy 0.924763\n",
      "MarginLoss + RegLoss: 0.28643823 + 0.15466088 = 0.4410991\n",
      "\n",
      "Epoch Number: 95\n",
      "Train Loss: 0.24117245918346775 Train accuracy: 0.9786111190915108\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: 0.2749009 + 0.15751003 = 0.43241096\n",
      "\n",
      "Epoch Number: 96\n",
      "Train Loss: 0.23339683355556595 Train accuracy: 0.9820833437972598\n",
      "Test accuracy 0.931739\n",
      "MarginLoss + RegLoss: 0.26679292 + 0.15683204 = 0.42362496\n",
      "\n",
      "Epoch Number: 97\n",
      "Train Loss: 0.22924452647566795 Train accuracy: 0.9829166763358645\n",
      "Test accuracy 0.930244\n",
      "MarginLoss + RegLoss: 0.26700675 + 0.15570298 = 0.42270973\n",
      "\n",
      "Epoch Number: 98\n",
      "Train Loss: 0.22590422609614003 Train accuracy: 0.9840277863873376\n",
      "Test accuracy 0.929248\n",
      "MarginLoss + RegLoss: 0.27022278 + 0.1549959 = 0.4252187\n",
      "\n",
      "Epoch Number: 99\n",
      "Train Loss: 0.2249646960861153 Train accuracy: 0.9841666751437717\n",
      "Test accuracy 0.930742\n",
      "MarginLoss + RegLoss: 0.2714921 + 0.15411127 = 0.42560336\n",
      "\n",
      "Maximum Test accuracy at compressed model size(including early stopping): 0.9352267 at Epoch: 33\n",
      "Final Test Accuracy: 0.9307424\n"
     ]
    }
   ],
   "source": [
    "totalEpochs = 100\n",
    "batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n",
    "\n",
    "bonsaiTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest, Ytrain, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_zero_ratios(tree):\n",
    "    zs = np.sum(np.abs(tree.Z.eval())<0.000000000000001)/(tree.Z.eval()!=None).sum()\n",
    "    ws = np.sum(np.abs(tree.W.eval())<0.000000000000001)/(tree.W.eval()!=None).sum()\n",
    "    vs = np.sum(np.abs(tree.V.eval())<0.000000000000001)/(tree.V.eval()!=None).sum()\n",
    "    ts = np.sum(np.abs(tree.T.eval())<0.000000000000001)/(tree.T.eval()!=None).sum()\n",
    "    print('Sparse ratios achieved...\\nW:',1-ws,'\\nV:',1-vs,'\\nT:',1-ts,'\\nZ:',1-zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse ratios achieved...\n",
      "W: 0.30000000000000004 \n",
      "V: 0.30000000000000004 \n",
      "T: 0.6173469387755102 \n",
      "Z: 0.20011117287381874\n"
     ]
    }
   ],
   "source": [
    "calc_zero_ratios(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# thankyou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
